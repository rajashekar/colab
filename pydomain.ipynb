{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajashekar/colab/blob/main/pydomain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pkXoBx-48-3y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import regularizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCiVQgJxLbCo"
      },
      "source": [
        "# Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "le9h8HDQ7LAY",
        "outputId": "635b2c08-8bac-4684-feb9-47eb1d6603c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jHonUH_D7f7R"
      },
      "outputs": [],
      "source": [
        "%cp /content/drive/MyDrive/Colab/pydomains/data/domain_final.zip /tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBfr7i9p76CP",
        "outputId": "a5185870-0879-435e-e151-0721fd8a7c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp\n"
          ]
        }
      ],
      "source": [
        "%cd /tmp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip domain_final.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4pQk6Dtkeku",
        "outputId": "e315094a-8f27-43ee-ea38-01603b375e4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  domain_final.zip\n",
            "  inflating: domain_final.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPYD-KpJ8BkS"
      },
      "outputs": [],
      "source": [
        "# !gzip -d shallalist.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpPP7-D98x6d",
        "outputId": "ffe6b748-404c-4b7f-8054-720a5f55dcf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61326 domain_final.csv\n"
          ]
        }
      ],
      "source": [
        "!wc -l domain_final.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wfbekxXH9A_m"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('domain_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nmL8ec_49J7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "894e1239-c6c6-4c85-f87f-5ead4f5b7605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text        domain_names  \\\n",
              "0  total home about right now assorted chocolate ...              kizito   \n",
              "1  rule there is of no curiosity the june biggest...   rule--34.blogspot   \n",
              "2  home mission statement of faith login harvest ...  agapeharvestchurch   \n",
              "3  western get instant and send money on the send...     westernunion.co   \n",
              "4  new view more view more new view more alfa alf...        alfaromeo-jp   \n",
              "\n",
              "          cat_name  \n",
              "0         shopping  \n",
              "1             porn  \n",
              "2         religion  \n",
              "3    finance/other  \n",
              "4  automobile/cars  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3874a389-901e-410b-aac3-b61f1f402b02\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>domain_names</th>\n",
              "      <th>cat_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>total home about right now assorted chocolate ...</td>\n",
              "      <td>kizito</td>\n",
              "      <td>shopping</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rule there is of no curiosity the june biggest...</td>\n",
              "      <td>rule--34.blogspot</td>\n",
              "      <td>porn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>home mission statement of faith login harvest ...</td>\n",
              "      <td>agapeharvestchurch</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>western get instant and send money on the send...</td>\n",
              "      <td>westernunion.co</td>\n",
              "      <td>finance/other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>new view more view more new view more alfa alf...</td>\n",
              "      <td>alfaromeo-jp</td>\n",
              "      <td>automobile/cars</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3874a389-901e-410b-aac3-b61f1f402b02')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3874a389-901e-410b-aac3-b61f1f402b02 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3874a389-901e-410b-aac3-b61f1f402b02');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QyNuvgOj9ODh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd3bae6-a8a9-490b-fdec-ca435e8ac5d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['shopping', 'porn', 'religion', 'finance/other', 'automobile/cars',\n",
              "       'forum', 'gamble', 'news', 'recreation/sports',\n",
              "       'recreation/travel', 'hobby/games-online', 'education/schools',\n",
              "       'music', 'chat', 'hobby/pets', 'finance/moneylending',\n",
              "       'finance/insurance', 'finance/banking', 'radiotv',\n",
              "       'recreation/martialarts', 'politics', 'hobby/games-misc',\n",
              "       'automobile/bikes', 'science/chemistry', 'drugs', 'adv', 'webmail',\n",
              "       'downloads', 'webradio', 'hospitals', 'government', 'warez',\n",
              "       'redirector', 'recreation/humor', 'hobby/cooking',\n",
              "       'recreation/restaurants', 'dating', 'dynamic', 'models',\n",
              "       'jobsearch', 'alcohol', 'recreation/wellness',\n",
              "       'finance/realestate', 'movies', 'spyware', 'military', 'weapons',\n",
              "       'automobile/planes', 'homestyle', 'library', 'fortunetelling',\n",
              "       'tracker', 'remotecontrol', 'sex/lingerie', 'violence',\n",
              "       'searchengines', 'ringtones', 'isp', 'aggressive',\n",
              "       'automobile/boats', 'imagehosting', 'science/astronomy',\n",
              "       'hobby/gardening', 'urlshortener', 'finance/trading', 'hacking',\n",
              "       'socialnet', 'sex/education', 'webphone', 'costtraps', 'podcasts',\n",
              "       'anonvpn', 'webtv', 'updatesites'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df['cat_name'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kzQCwPIuf1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47a2ba1-a00e-4433-8e4e-34a3e0258567"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(df['cat_name'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bFHbriSL9ng4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "06b5f91e-7666-4d00-919d-7e1494eb0aa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  text  domain_names\n",
              "cat_name                            \n",
              "adv               1920          1920\n",
              "aggressive          91            91\n",
              "alcohol            336           336\n",
              "anonvpn             50            50\n",
              "automobile/bikes   312           312\n",
              "...                ...           ...\n",
              "weapons            153           153\n",
              "webmail            444           444\n",
              "webphone            23            23\n",
              "webradio           370           370\n",
              "webtv               57            57\n",
              "\n",
              "[74 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1419d90-2329-4689-b401-7b458e56ac7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>domain_names</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>adv</th>\n",
              "      <td>1920</td>\n",
              "      <td>1920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aggressive</th>\n",
              "      <td>91</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alcohol</th>\n",
              "      <td>336</td>\n",
              "      <td>336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anonvpn</th>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>automobile/bikes</th>\n",
              "      <td>312</td>\n",
              "      <td>312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weapons</th>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webmail</th>\n",
              "      <td>444</td>\n",
              "      <td>444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webphone</th>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webradio</th>\n",
              "      <td>370</td>\n",
              "      <td>370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webtv</th>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1419d90-2329-4689-b401-7b458e56ac7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1419d90-2329-4689-b401-7b458e56ac7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1419d90-2329-4689-b401-7b458e56ac7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df.groupby('cat_name').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cRTrrrgihxu4"
      },
      "outputs": [],
      "source": [
        "df['category_codes'] = df.cat_name.astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hgLVwoM3kYlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a512a5d8-2739-4b33-cdb7-75e8a63c87d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['adv', 'aggressive', 'alcohol', 'anonvpn', 'automobile/bikes',\n",
              "       'automobile/boats', 'automobile/cars', 'automobile/planes', 'chat',\n",
              "       'costtraps', 'dating', 'downloads', 'drugs', 'dynamic',\n",
              "       'education/schools', 'finance/banking', 'finance/insurance',\n",
              "       'finance/moneylending', 'finance/other', 'finance/realestate',\n",
              "       'finance/trading', 'fortunetelling', 'forum', 'gamble', 'government',\n",
              "       'hacking', 'hobby/cooking', 'hobby/games-misc', 'hobby/games-online',\n",
              "       'hobby/gardening', 'hobby/pets', 'homestyle', 'hospitals',\n",
              "       'imagehosting', 'isp', 'jobsearch', 'library', 'military', 'models',\n",
              "       'movies', 'music', 'news', 'podcasts', 'politics', 'porn', 'radiotv',\n",
              "       'recreation/humor', 'recreation/martialarts', 'recreation/restaurants',\n",
              "       'recreation/sports', 'recreation/travel', 'recreation/wellness',\n",
              "       'redirector', 'religion', 'remotecontrol', 'ringtones',\n",
              "       'science/astronomy', 'science/chemistry', 'searchengines',\n",
              "       'sex/education', 'sex/lingerie', 'shopping', 'socialnet', 'spyware',\n",
              "       'tracker', 'updatesites', 'urlshortener', 'violence', 'warez',\n",
              "       'weapons', 'webmail', 'webphone', 'webradio', 'webtv'],\n",
              "      dtype='object', name='cat_name')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df.groupby('cat_name').count().index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = df.groupby('cat_name').count().index"
      ],
      "metadata": {
        "id": "kRuw64ftTWKF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('category_codes').count().index"
      ],
      "metadata": {
        "id": "St75KJhtM7XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd894ae-2b96-4877-b393-d740e7668251"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "            51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "            68, 69, 70, 71, 72, 73],\n",
              "           dtype='int64', name='category_codes')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['category_codes'] == 72]"
      ],
      "metadata": {
        "id": "Am7kt4XAUWqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "fe1cf92d-3ebf-4d88-809c-1a33650587fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  \\\n",
              "64     toggle navigation all streaming live streaming...   \n",
              "116    toggle search form search toggle navigation ho...   \n",
              "235    listen wake up call bill and and ken coast to ...   \n",
              "284    are you a fan of check out the welcome to the ...   \n",
              "347    radio simple for simple people radio seit sind...   \n",
              "...                                                  ...   \n",
              "60239  home splash splash reality a da de em da me pl...   \n",
              "60319  twitter home current stairway stairway with tr...   \n",
              "60465  is currently off the please check back to this...   \n",
              "60646  skip to content no de de no listen web rock ra...   \n",
              "61288  join home tour help lost password join hosting...   \n",
              "\n",
              "                      domain_names  cat_name  category_codes  \n",
              "64                 wyep.streamguys  webradio              72  \n",
              "116                      wgmuradio  webradio              72  \n",
              "235                       kfiam640  webradio              72  \n",
              "284                  radio-locator  webradio              72  \n",
              "347                     radiosucks  webradio              72  \n",
              "...                            ...       ...             ...  \n",
              "60239         radio.musica.uol.com  webradio              72  \n",
              "60319                  ghostlytalk  webradio              72  \n",
              "60465  shoutbrothershout.homestead  webradio              72  \n",
              "60646                 webrockradio  webradio              72  \n",
              "61288         darkradio.mybravenet  webradio              72  \n",
              "\n",
              "[370 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1db9d4e-f5ef-41be-bf0f-6f03b816c0fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>domain_names</th>\n",
              "      <th>cat_name</th>\n",
              "      <th>category_codes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>toggle navigation all streaming live streaming...</td>\n",
              "      <td>wyep.streamguys</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>toggle search form search toggle navigation ho...</td>\n",
              "      <td>wgmuradio</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>listen wake up call bill and and ken coast to ...</td>\n",
              "      <td>kfiam640</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>are you a fan of check out the welcome to the ...</td>\n",
              "      <td>radio-locator</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>radio simple for simple people radio seit sind...</td>\n",
              "      <td>radiosucks</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60239</th>\n",
              "      <td>home splash splash reality a da de em da me pl...</td>\n",
              "      <td>radio.musica.uol.com</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60319</th>\n",
              "      <td>twitter home current stairway stairway with tr...</td>\n",
              "      <td>ghostlytalk</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60465</th>\n",
              "      <td>is currently off the please check back to this...</td>\n",
              "      <td>shoutbrothershout.homestead</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60646</th>\n",
              "      <td>skip to content no de de no listen web rock ra...</td>\n",
              "      <td>webrockradio</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61288</th>\n",
              "      <td>join home tour help lost password join hosting...</td>\n",
              "      <td>darkradio.mybravenet</td>\n",
              "      <td>webradio</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>370 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1db9d4e-f5ef-41be-bf0f-6f03b816c0fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1db9d4e-f5ef-41be-bf0f-6f03b816c0fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1db9d4e-f5ef-41be-bf0f-6f03b816c0fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0RB76bLriCmD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8e6dd907-d252-462d-a5e9-c9595dd6bf3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text        domain_names  \\\n",
              "0  total home about right now assorted chocolate ...              kizito   \n",
              "1  rule there is of no curiosity the june biggest...   rule--34.blogspot   \n",
              "2  home mission statement of faith login harvest ...  agapeharvestchurch   \n",
              "3  western get instant and send money on the send...     westernunion.co   \n",
              "4  new view more view more new view more alfa alf...        alfaromeo-jp   \n",
              "\n",
              "          cat_name  category_codes  \n",
              "0         shopping              61  \n",
              "1             porn              44  \n",
              "2         religion              53  \n",
              "3    finance/other              18  \n",
              "4  automobile/cars               6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-662ebccf-eaf8-4814-b49c-1aac54f16a4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>domain_names</th>\n",
              "      <th>cat_name</th>\n",
              "      <th>category_codes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>total home about right now assorted chocolate ...</td>\n",
              "      <td>kizito</td>\n",
              "      <td>shopping</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rule there is of no curiosity the june biggest...</td>\n",
              "      <td>rule--34.blogspot</td>\n",
              "      <td>porn</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>home mission statement of faith login harvest ...</td>\n",
              "      <td>agapeharvestchurch</td>\n",
              "      <td>religion</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>western get instant and send money on the send...</td>\n",
              "      <td>westernunion.co</td>\n",
              "      <td>finance/other</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>new view more view more new view more alfa alf...</td>\n",
              "      <td>alfaromeo-jp</td>\n",
              "      <td>automobile/cars</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-662ebccf-eaf8-4814-b49c-1aac54f16a4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-662ebccf-eaf8-4814-b49c-1aac54f16a4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-662ebccf-eaf8-4814-b49c-1aac54f16a4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bKzl4wwrDHK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebcb7e57-9261-4460-db95-7c466df0709a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        kizito total home about right now assorted cho...\n",
              "1        rule--34.blogspot rule there is of no curiosit...\n",
              "2        agapeharvestchurch home mission statement of f...\n",
              "3        westernunion.co western get instant and send m...\n",
              "4        alfaromeo-jp new view more view more new view ...\n",
              "                               ...                        \n",
              "61320    astro.elte information staff education researc...\n",
              "61321    clarkjoneskarate sign in to control panel secu...\n",
              "61322    inflagranti-music news rock pop dance party di...\n",
              "61323    euribor the institute about the institute disc...\n",
              "61324    unsw.edu skip to main content study study expl...\n",
              "Length: 61325, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df['domain_names'] + ' ' + df['text'] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_of_text = df['text'].str.split(\"\\\\s+\")"
      ],
      "metadata": {
        "id": "b2BPXMRxKxRp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_of_text.str.len().max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whmStbjxX1O-",
        "outputId": "5f154fa1-8a7c-4b51-ba5b-310aaf198637"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15481712"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zk-WdFBpHrxP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "4a8053e6-799a-4463-d64f-601a5af74b49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text       domain_names  \\\n",
              "9      listen on air bosch schedule recently top radi...            wxra945   \n",
              "33     music music radio par format se continuer la m...          radio.m24   \n",
              "39     cover letter resume is a nationally columnist ...      ariannaonline   \n",
              "41     league de league de a la candela y hay de al t...              rpctv   \n",
              "46     and specialist franco magazine test sport tag ...  giornaledellavela   \n",
              "...                                                  ...                ...   \n",
              "61233  tip de van en business sport folklore register...   zwartewaterkrant   \n",
              "61239  listen live listen to east coast gold big dona...             ecr.co   \n",
              "61261  league en de el bolivia red costa el canal cha...         canal7.com   \n",
              "61312  na a o na se se za a sa ale po na v die do v a...          noveslovo   \n",
              "61317  log out are you the owner of this tripod the t...     sassone.tripod   \n",
              "\n",
              "      cat_name  category_codes  \n",
              "9         news              41  \n",
              "33        news              41  \n",
              "39        news              41  \n",
              "41        news              41  \n",
              "46        news              41  \n",
              "...        ...             ...  \n",
              "61233     news              41  \n",
              "61239     news              41  \n",
              "61261     news              41  \n",
              "61312     news              41  \n",
              "61317     news              41  \n",
              "\n",
              "[5126 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8cdb96cd-9e85-41f7-bdb4-256da2bc67a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>domain_names</th>\n",
              "      <th>cat_name</th>\n",
              "      <th>category_codes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>listen on air bosch schedule recently top radi...</td>\n",
              "      <td>wxra945</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>music music radio par format se continuer la m...</td>\n",
              "      <td>radio.m24</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>cover letter resume is a nationally columnist ...</td>\n",
              "      <td>ariannaonline</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>league de league de a la candela y hay de al t...</td>\n",
              "      <td>rpctv</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>and specialist franco magazine test sport tag ...</td>\n",
              "      <td>giornaledellavela</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61233</th>\n",
              "      <td>tip de van en business sport folklore register...</td>\n",
              "      <td>zwartewaterkrant</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61239</th>\n",
              "      <td>listen live listen to east coast gold big dona...</td>\n",
              "      <td>ecr.co</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61261</th>\n",
              "      <td>league en de el bolivia red costa el canal cha...</td>\n",
              "      <td>canal7.com</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61312</th>\n",
              "      <td>na a o na se se za a sa ale po na v die do v a...</td>\n",
              "      <td>noveslovo</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61317</th>\n",
              "      <td>log out are you the owner of this tripod the t...</td>\n",
              "      <td>sassone.tripod</td>\n",
              "      <td>news</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5126 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cdb96cd-9e85-41f7-bdb4-256da2bc67a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cdb96cd-9e85-41f7-bdb4-256da2bc67a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cdb96cd-9e85-41f7-bdb4-256da2bc67a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "df.loc[df['cat_name'] == 'news']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqBTCPJqLhi4"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3FGAY-nWIJOU"
      },
      "outputs": [],
      "source": [
        "X = (df['domain_names'] + ' ' + df['text']).values\n",
        "y = df['category_codes'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EXaMwNI_fLY9"
      },
      "outputs": [],
      "source": [
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liK0NkVFfami",
        "outputId": "34ab5bfb-9e0f-4271-b3b7-21f04cdff770"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42927"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "X_train.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bCFCtxzFnB69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf0911e-f78e-45ca-d5eb-a82baea6eb63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "        68, 69, 70, 71, 72, 73], dtype=int8),\n",
              " array([1337,   57,  238,   41,  210,   96,  718,  174, 2276,   37,  511,\n",
              "         481,  581,  260, 2541, 1546,  646,  353,  153,  433,   81,  242,\n",
              "        1480, 1274,  183,  101,  204,  499, 1831,  257, 1714,   70,  526,\n",
              "         113,  201,  964,   52,   52,  345, 1095, 1420, 3603,   23,  485,\n",
              "        1905, 1056,  203,  490,  377, 2308, 2150,  139,  220, 1380,   14,\n",
              "          67,  270,   59,  111,   38,  130,  956,  111,  253,  200,   38,\n",
              "          90,   20,  104,  112,  311,   13,  259,   39]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "np.unique(y_train, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DO54iaV1ffVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644c28bd-9bb2-446e-d68b-5400fd9afd32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18398"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "X_rem.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxefFkqSnTPq",
        "outputId": "c7005c7a-c853-4148-a2d0-9547e5248da0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "        68, 69, 70, 71, 72, 73], dtype=int8),\n",
              " array([ 583,   34,   98,    9,  102,   28,  328,   85,  969,    9,  222,\n",
              "         229,  223,  100, 1107,  685,  298,  145,   76,  165,   37,   95,\n",
              "         640,  583,   81,   37,   80,  235,  817,   98,  769,   22,  257,\n",
              "          29,   95,  378,   25,   17,  169,  464,  618, 1523,   13,  206,\n",
              "         747,  448,   92,  222,  170,  946,  912,   51,   96,  591,    3,\n",
              "          18,  140,   20,   46,   17,   59,  392,   47,  107,   74,   15,\n",
              "          34,    7,   48,   41,  133,   10,  111,   18]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "np.unique(y_rem, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lVtrJoJPfgp6"
      },
      "outputs": [],
      "source": [
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wO_mgdWHfjz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b57a512-3854-49cf-9e42-14b299a0c3a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9199"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "X_val.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NDQa1kjLnaI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ade58ac-a5fa-4622-96b3-4895c9a219f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "        69, 70, 71, 72, 73], dtype=int8),\n",
              " array([291,  15,  50,   3,  59,  18, 165,  44, 479,   4, 111, 111, 102,\n",
              "         46, 536, 350, 134,  72,  46,  94,  16,  39, 309, 289,  41,  24,\n",
              "         41, 123, 375,  53, 405,  10, 133,  14,  50, 200,  14,   9,  93,\n",
              "        221, 309, 770,   5,  99, 368, 227,  47, 109,  84, 482, 472,  26,\n",
              "         61, 304,  14,  63,   9,  22,  11,  31, 184,  24,  48,  38,   5,\n",
              "         18,   5,  23,  23,  68,   5,  45,  11]))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "np.unique(y_val, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSqmj4AnfmFT",
        "outputId": "81d8f4c6-27eb-46f4-a496-2f806d7f2b2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9199"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "X_test.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "qVc_YQyFnxnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c8ec4b-8635-4556-a3ad-68a396440365"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "        68, 69, 70, 71, 72, 73], dtype=int8),\n",
              " array([292,  19,  48,   6,  43,  10, 163,  41, 490,   5, 111, 118, 121,\n",
              "         54, 571, 335, 164,  73,  30,  71,  21,  56, 331, 294,  40,  13,\n",
              "         39, 112, 442,  45, 364,  12, 124,  15,  45, 178,  11,   8,  76,\n",
              "        243, 309, 753,   8, 107, 379, 221,  45, 113,  86, 464, 440,  25,\n",
              "         35, 287,   3,   4,  77,  11,  24,   6,  28, 208,  23,  59,  36,\n",
              "         10,  16,   2,  25,  18,  65,   5,  66,   7]))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "np.unique(y_test, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "sgpypwmbimVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "e12d874d-e580-4be7-e4a8-c92655003191"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bmwparklane menu control park lane book a test drive book a service home finance and used business and contact us about the series coupe is find out close welcome to park the flagship offering nationwide used vehicle search your used vehicle search your new car search now used car search now news new car find out more martin general manager the journey to your next as simple and convenient for you as news park lane read more promotion the active tourer be one of the first to book a test and receive a voucher courtesy of red letter read more discover the of a electric or hybrid vehicle and calculate your potential fuel find out more explore our with our range now boasting more than ever finding the perfect for you never been find out more your whether a an accident a new accessory or simply advice you we can how can we help you business and fleet business company car a strong range of and thanks to highly and comprehensive combined with low running as a result of fuel and find out more about us or in person here to help contact us book an appointment about park find out more are you ready to join the winning recruitment follow us twitter address park park talk to us contact contact us form book an appointment quick links park lane park lane legal information and motor industry code of practice brand protection modern slavery statement product safety enquiry park lane'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "X_train[10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "j4F4Z2OgfonN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac045832-fa89-4252-e5ef-324084d7abaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "y_train[10000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['domain_names'].str.contains('bmwparklane')]"
      ],
      "metadata": {
        "id": "0JiQ9ZzqLJhQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "f2f8c8b2-a578-4cc4-e79e-1185d0d06b1f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text domain_names  \\\n",
              "37128  menu control park lane book a test drive book ...  bmwparklane   \n",
              "\n",
              "              cat_name  category_codes  \n",
              "37128  automobile/cars               6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a396cd4-6297-43a3-80f1-11c912285167\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>domain_names</th>\n",
              "      <th>cat_name</th>\n",
              "      <th>category_codes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37128</th>\n",
              "      <td>menu control park lane book a test drive book ...</td>\n",
              "      <td>bmwparklane</td>\n",
              "      <td>automobile/cars</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a396cd4-6297-43a3-80f1-11c912285167')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a396cd4-6297-43a3-80f1-11c912285167 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a396cd4-6297-43a3-80f1-11c912285167');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "86rTnR99foJH"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train, len(df['category_codes'].unique()))\n",
        "y_val = to_categorical(y_val, len(df['category_codes'].unique()))\n",
        "y_test = to_categorical(y_test, len(df['category_codes'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oLa7mRLPowiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a312865-1ff2-44a5-fdcb-566d9b1e175c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "y_train[10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBYwzaeipDII"
      },
      "source": [
        "# Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oY8MczDMo2RH"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 600000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GAVNgehHpQQs"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lS7Ews4pSmf",
        "outputId": "4aff78cc-7990-4300-aa61-da7f81bb53c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name tf.Tensor(b'sporn skip to content close menu shop our story which harness is best for your log in twitter cart translation for safety please allow extra time for to be particularly international shop our story which harness is best for your log in search site navigation cart search translation translation does your dog pull the most popular and best selling pull control since shop now stop the most popular and best selling pull control since shop now for the ultimate in pull control the ultimate control learn more stop the most popular and best selling pull control since shop now check out our new line of aquarium and much shop now over million sold our top view all add to cart ultimate harness from add to cart mesh from add to cart original from view all ultimate harness regular price the ultimate control dog harness is the latest addition to our line of control offering of pull control in one comfort design the dog owner several ways to humanely control their dog when on the the harness is easy to put on and is to start working without intensive key front pull control option rear pull control option simultaneous front and rear control easy grab emergency handle detachable accessory extension throughout for comfort adjustable around the chest and girth lifetime guarantee sizing information girth of or dogs approximately girth of or dogs approximately girth of or dogs approximately instructional for walking do not leave on dog not designed for use as a car harness or patent us customer based on write a review mesh regular price the mesh dog like a glove and is designed to curb moderate to heavy leash made for comfort and this harness an mesh chest piece that with your designed to humanely control any size dog without and the harness is easy to put on and take the restraint offer extra comfort under the front key system with your natural designed to stop the humanely without choking restraint allow for a comfy fit under front structure made with the highest quality nylon sizing information inch or dogs approximately inch neck or dogs approximately inch neck or dogs approximately inch neck or dogs approximately for walking do not leave on dog not designed for use as a car harness or patent us customer based on write a review original regular price the original training for dogs is designed for moderate to heavy and is to stop your dog from the made of high braided cord and nylon with thermoplastic and steel to reduce its provide maximum comfort and prevent underarm system instantly the to a standard collar if the pull function is not for the lifetime of the dog key sizing information inch or dogs approximately inch or dogs approximately inch or dogs approximately inch or dogs approximately for walking do not leave on dog not designed for use as a car harness or patent us a customer based on write a review best the harness stop your dog from a super comfortable mesh design as well as for system with your natural buy now ultimate control the ultimate control harness for dogs is the latest addition to our line of pull control the ultimate control a comfortable design with of pull control in buy now is your dog a big durable marrow available in a wide variety of and the marrow are to satisfy even the of jerky flavor down the middle is an extra buy now this is since we got the our shepherd learned how to walk properly without and the moment she she immediately and slows her pace to match we love the ultimate control worked on our great the very first time we tried we can now safely walk him without him dragging us down the safe for him unlike a choker this is by far the best thing i ever our jack russel terrier is a ball of and so hard his tongue turns i tried the mesh harness today for the first time and it worked like a twitter search shipping privacy policy map policy contact main menu main menu shop our story which harness is best for your sign up and save sign up and save subscribe to get special free and enter your twitter new peanut butter flavored marrow check out our brand new peanut butter flavored marrow while available in sizes to suit every size shop now', shape=(), dtype=string)\n",
            "Label tf.Tensor(\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.], shape=(74,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "text_batch, label_batch = next(iter(train_dataset))\n",
        "site_name, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Name\", site_name)\n",
        "print(\"Label\", first_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQgvFe5RpyCb"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "KgJH_EOXpicC"
      },
      "outputs": [],
      "source": [
        "max_features = 62000\n",
        "sequence_length = 5000\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "SyqUpEcLp3l6"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = train_dataset.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NjhNiD-lp59L"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIlxukuzqAxb",
        "outputId": "f83036ac-12da-4aeb-a58c-5921705d7c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized data (<tf.Tensor: shape=(1, 5000), dtype=int64, numpy=array([[42610,   671,     5, ...,     0,     0,     0]])>, <tf.Tensor: shape=(74,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0.], dtype=float32)>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Vectorized data\", vectorize_text(site_name, first_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyBso3wLqFcs",
        "outputId": "03a303bb-357c-4263-a813-506aaff3f96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 --->  girl\n",
            "Vocabulary size: 62000\n"
          ]
        }
      ],
      "source": [
        "print(\"1000 ---> \",vectorize_layer.get_vocabulary()[1000])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yakBvEqrqzH0"
      },
      "outputs": [],
      "source": [
        "train_ds = train_dataset.map(vectorize_text)\n",
        "val_ds = val_dataset.map(vectorize_text)\n",
        "test_ds = test_dataset.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "n86acSL9q7Wg"
      },
      "outputs": [],
      "source": [
        "# For performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxTAfTAMq_gN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVyTsboEq9hL",
        "outputId": "c017d25d-1046-4acc-b61c-6b5cd5027a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 64)          3968064   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 64)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 64)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 74)                4810      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,972,874\n",
            "Trainable params: 3,972,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 64\n",
        "weight_decay = 0.001\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(max_features + 1, embedding_dim),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(len(df['category_codes'].unique()), \n",
        "                        activation='softmax', \n",
        "                        kernel_regularizer=regularizers.L1L2(l1=weight_decay, l2=weight_decay),\n",
        "                        bias_regularizer=regularizers.L2(weight_decay),\n",
        "                        activity_regularizer=regularizers.L2(weight_decay))])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "u-m02mAEqciz"
      },
      "outputs": [],
      "source": [
        "model_save_filename = \"model.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "7iLAfw29rCeG"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 500\n",
        "initial_learning_rate = 0.006\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "earlystopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_accuracy',\n",
        "    mode = 'max',\n",
        "    verbose = 1,\n",
        "    patience = 10,\n",
        "    restore_best_weights = True\n",
        ")\n",
        "\n",
        "mdlcheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_filename,\n",
        "    monitor = 'val_accuracy',\n",
        "    mode = 'max',\n",
        "    verbose = 1,\n",
        "    save_best_only = True\n",
        ")\n",
        "\n",
        "decay = initial_learning_rate / epochs\n",
        "\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "  return lr * 1/ (1 + decay * epoch)\n",
        "\n",
        "lrscheduler_cb = tf.keras.callbacks.LearningRateScheduler(lr_time_based_decay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85PC11W1rTYS",
        "outputId": "64d68be6-66d2-48d9-bd2f-f3f55925accd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.8330 - accuracy: 0.0808\n",
            "Epoch 1: val_accuracy improved from -inf to 0.08381, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.8319 - accuracy: 0.0809 - val_loss: 3.7720 - val_accuracy: 0.0838 - lr: 0.0060\n",
            "Epoch 2/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.7439 - accuracy: 0.0938\n",
            "Epoch 2: val_accuracy improved from 0.08381 to 0.11262, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.7430 - accuracy: 0.0941 - val_loss: 3.7241 - val_accuracy: 0.1126 - lr: 0.0060\n",
            "Epoch 3/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.6996 - accuracy: 0.1135\n",
            "Epoch 3: val_accuracy improved from 0.11262 to 0.12338, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.6986 - accuracy: 0.1137 - val_loss: 3.6886 - val_accuracy: 0.1234 - lr: 0.0060\n",
            "Epoch 4/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.6711 - accuracy: 0.1273\n",
            "Epoch 4: val_accuracy improved from 0.12338 to 0.13588, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.6702 - accuracy: 0.1276 - val_loss: 3.6610 - val_accuracy: 0.1359 - lr: 0.0060\n",
            "Epoch 5/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.6444 - accuracy: 0.1398\n",
            "Epoch 5: val_accuracy improved from 0.13588 to 0.14643, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.6434 - accuracy: 0.1399 - val_loss: 3.6369 - val_accuracy: 0.1464 - lr: 0.0060\n",
            "Epoch 6/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.6199 - accuracy: 0.1503\n",
            "Epoch 6: val_accuracy improved from 0.14643 to 0.15610, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.6191 - accuracy: 0.1503 - val_loss: 3.6124 - val_accuracy: 0.1561 - lr: 0.0060\n",
            "Epoch 7/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.5937 - accuracy: 0.1642\n",
            "Epoch 7: val_accuracy improved from 0.15610 to 0.16828, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.5927 - accuracy: 0.1644 - val_loss: 3.5897 - val_accuracy: 0.1683 - lr: 0.0060\n",
            "Epoch 8/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.5690 - accuracy: 0.1797\n",
            "Epoch 8: val_accuracy improved from 0.16828 to 0.18535, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.5681 - accuracy: 0.1800 - val_loss: 3.5642 - val_accuracy: 0.1853 - lr: 0.0060\n",
            "Epoch 9/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.5446 - accuracy: 0.1927\n",
            "Epoch 9: val_accuracy improved from 0.18535 to 0.20274, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.5438 - accuracy: 0.1930 - val_loss: 3.5430 - val_accuracy: 0.2027 - lr: 0.0060\n",
            "Epoch 10/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.5226 - accuracy: 0.2076\n",
            "Epoch 10: val_accuracy improved from 0.20274 to 0.21220, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.5217 - accuracy: 0.2078 - val_loss: 3.5232 - val_accuracy: 0.2122 - lr: 0.0060\n",
            "Epoch 11/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.5000 - accuracy: 0.2227\n",
            "Epoch 11: val_accuracy improved from 0.21220 to 0.23057, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.4991 - accuracy: 0.2228 - val_loss: 3.5033 - val_accuracy: 0.2306 - lr: 0.0060\n",
            "Epoch 12/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.4837 - accuracy: 0.2343\n",
            "Epoch 12: val_accuracy improved from 0.23057 to 0.23763, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.4828 - accuracy: 0.2345 - val_loss: 3.4848 - val_accuracy: 0.2376 - lr: 0.0060\n",
            "Epoch 13/500\n",
            "669/671 [============================>.] - ETA: 0s - loss: 3.4648 - accuracy: 0.2443\n",
            "Epoch 13: val_accuracy improved from 0.23763 to 0.26035, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.4644 - accuracy: 0.2443 - val_loss: 3.4714 - val_accuracy: 0.2604 - lr: 0.0060\n",
            "Epoch 14/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 3.4443 - accuracy: 0.2542\n",
            "Epoch 14: val_accuracy improved from 0.26035 to 0.27068, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.4440 - accuracy: 0.2542 - val_loss: 3.4531 - val_accuracy: 0.2707 - lr: 0.0060\n",
            "Epoch 15/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.4274 - accuracy: 0.2611\n",
            "Epoch 15: val_accuracy improved from 0.27068 to 0.27883, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.4265 - accuracy: 0.2614 - val_loss: 3.4342 - val_accuracy: 0.2788 - lr: 0.0060\n",
            "Epoch 16/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.4098 - accuracy: 0.2694\n",
            "Epoch 16: val_accuracy improved from 0.27883 to 0.27949, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.4090 - accuracy: 0.2695 - val_loss: 3.4193 - val_accuracy: 0.2795 - lr: 0.0060\n",
            "Epoch 17/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3931 - accuracy: 0.2764\n",
            "Epoch 17: val_accuracy improved from 0.27949 to 0.28699, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3921 - accuracy: 0.2767 - val_loss: 3.4042 - val_accuracy: 0.2870 - lr: 0.0060\n",
            "Epoch 18/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3770 - accuracy: 0.2848\n",
            "Epoch 18: val_accuracy improved from 0.28699 to 0.29286, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3761 - accuracy: 0.2849 - val_loss: 3.3915 - val_accuracy: 0.2929 - lr: 0.0060\n",
            "Epoch 19/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3623 - accuracy: 0.2898\n",
            "Epoch 19: val_accuracy improved from 0.29286 to 0.29927, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3615 - accuracy: 0.2899 - val_loss: 3.3760 - val_accuracy: 0.2993 - lr: 0.0060\n",
            "Epoch 20/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3467 - accuracy: 0.2967\n",
            "Epoch 20: val_accuracy improved from 0.29927 to 0.30329, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3459 - accuracy: 0.2968 - val_loss: 3.3650 - val_accuracy: 0.3033 - lr: 0.0060\n",
            "Epoch 21/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3354 - accuracy: 0.3027\n",
            "Epoch 21: val_accuracy improved from 0.30329 to 0.31362, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3347 - accuracy: 0.3029 - val_loss: 3.3529 - val_accuracy: 0.3136 - lr: 0.0060\n",
            "Epoch 22/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3225 - accuracy: 0.3058\n",
            "Epoch 22: val_accuracy improved from 0.31362 to 0.31590, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3215 - accuracy: 0.3061 - val_loss: 3.3418 - val_accuracy: 0.3159 - lr: 0.0060\n",
            "Epoch 23/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.3080 - accuracy: 0.3113\n",
            "Epoch 23: val_accuracy did not improve from 0.31590\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.3071 - accuracy: 0.3117 - val_loss: 3.3323 - val_accuracy: 0.3158 - lr: 0.0060\n",
            "Epoch 24/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 3.2981 - accuracy: 0.3140\n",
            "Epoch 24: val_accuracy improved from 0.31590 to 0.32862, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2977 - accuracy: 0.3143 - val_loss: 3.3190 - val_accuracy: 0.3286 - lr: 0.0060\n",
            "Epoch 25/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2851 - accuracy: 0.3215\n",
            "Epoch 25: val_accuracy improved from 0.32862 to 0.33525, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2843 - accuracy: 0.3216 - val_loss: 3.3089 - val_accuracy: 0.3353 - lr: 0.0060\n",
            "Epoch 26/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2732 - accuracy: 0.3236\n",
            "Epoch 26: val_accuracy improved from 0.33525 to 0.33873, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2724 - accuracy: 0.3237 - val_loss: 3.2985 - val_accuracy: 0.3387 - lr: 0.0060\n",
            "Epoch 27/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2625 - accuracy: 0.3281\n",
            "Epoch 27: val_accuracy did not improve from 0.33873\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2616 - accuracy: 0.3283 - val_loss: 3.2923 - val_accuracy: 0.3338 - lr: 0.0060\n",
            "Epoch 28/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2536 - accuracy: 0.3326\n",
            "Epoch 28: val_accuracy improved from 0.33873 to 0.34569, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2527 - accuracy: 0.3328 - val_loss: 3.2786 - val_accuracy: 0.3457 - lr: 0.0060\n",
            "Epoch 29/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2397 - accuracy: 0.3346\n",
            "Epoch 29: val_accuracy improved from 0.34569 to 0.34580, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2388 - accuracy: 0.3348 - val_loss: 3.2732 - val_accuracy: 0.3458 - lr: 0.0060\n",
            "Epoch 30/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2297 - accuracy: 0.3381\n",
            "Epoch 30: val_accuracy improved from 0.34580 to 0.35080, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2289 - accuracy: 0.3384 - val_loss: 3.2638 - val_accuracy: 0.3508 - lr: 0.0060\n",
            "Epoch 31/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 3.2214 - accuracy: 0.3401\n",
            "Epoch 31: val_accuracy did not improve from 0.35080\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2210 - accuracy: 0.3404 - val_loss: 3.2573 - val_accuracy: 0.3496 - lr: 0.0060\n",
            "Epoch 32/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2117 - accuracy: 0.3433\n",
            "Epoch 32: val_accuracy improved from 0.35080 to 0.35504, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2109 - accuracy: 0.3436 - val_loss: 3.2468 - val_accuracy: 0.3550 - lr: 0.0060\n",
            "Epoch 33/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.2038 - accuracy: 0.3483\n",
            "Epoch 33: val_accuracy improved from 0.35504 to 0.35689, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.2030 - accuracy: 0.3485 - val_loss: 3.2402 - val_accuracy: 0.3569 - lr: 0.0060\n",
            "Epoch 34/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1943 - accuracy: 0.3518\n",
            "Epoch 34: val_accuracy improved from 0.35689 to 0.36200, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1937 - accuracy: 0.3520 - val_loss: 3.2337 - val_accuracy: 0.3620 - lr: 0.0060\n",
            "Epoch 35/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1847 - accuracy: 0.3555\n",
            "Epoch 35: val_accuracy improved from 0.36200 to 0.36460, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1840 - accuracy: 0.3556 - val_loss: 3.2255 - val_accuracy: 0.3646 - lr: 0.0060\n",
            "Epoch 36/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1741 - accuracy: 0.3562\n",
            "Epoch 36: val_accuracy improved from 0.36460 to 0.37069, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1735 - accuracy: 0.3563 - val_loss: 3.2171 - val_accuracy: 0.3707 - lr: 0.0060\n",
            "Epoch 37/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1683 - accuracy: 0.3599\n",
            "Epoch 37: val_accuracy did not improve from 0.37069\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1676 - accuracy: 0.3601 - val_loss: 3.2124 - val_accuracy: 0.3674 - lr: 0.0060\n",
            "Epoch 38/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1579 - accuracy: 0.3630\n",
            "Epoch 38: val_accuracy did not improve from 0.37069\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1571 - accuracy: 0.3632 - val_loss: 3.2051 - val_accuracy: 0.3683 - lr: 0.0059\n",
            "Epoch 39/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1494 - accuracy: 0.3650\n",
            "Epoch 39: val_accuracy improved from 0.37069 to 0.37482, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1485 - accuracy: 0.3652 - val_loss: 3.1996 - val_accuracy: 0.3748 - lr: 0.0059\n",
            "Epoch 40/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1432 - accuracy: 0.3672\n",
            "Epoch 40: val_accuracy did not improve from 0.37482\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1425 - accuracy: 0.3673 - val_loss: 3.1970 - val_accuracy: 0.3722 - lr: 0.0059\n",
            "Epoch 41/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1333 - accuracy: 0.3690\n",
            "Epoch 41: val_accuracy did not improve from 0.37482\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1324 - accuracy: 0.3692 - val_loss: 3.1871 - val_accuracy: 0.3720 - lr: 0.0059\n",
            "Epoch 42/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1278 - accuracy: 0.3723\n",
            "Epoch 42: val_accuracy improved from 0.37482 to 0.37721, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1271 - accuracy: 0.3724 - val_loss: 3.1828 - val_accuracy: 0.3772 - lr: 0.0059\n",
            "Epoch 43/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1189 - accuracy: 0.3751\n",
            "Epoch 43: val_accuracy improved from 0.37721 to 0.37765, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1181 - accuracy: 0.3753 - val_loss: 3.1781 - val_accuracy: 0.3776 - lr: 0.0059\n",
            "Epoch 44/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1126 - accuracy: 0.3763\n",
            "Epoch 44: val_accuracy improved from 0.37765 to 0.38330, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1119 - accuracy: 0.3764 - val_loss: 3.1684 - val_accuracy: 0.3833 - lr: 0.0059\n",
            "Epoch 45/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.1051 - accuracy: 0.3795\n",
            "Epoch 45: val_accuracy did not improve from 0.38330\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.1043 - accuracy: 0.3797 - val_loss: 3.1666 - val_accuracy: 0.3771 - lr: 0.0059\n",
            "Epoch 46/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0977 - accuracy: 0.3818\n",
            "Epoch 46: val_accuracy did not improve from 0.38330\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0970 - accuracy: 0.3820 - val_loss: 3.1578 - val_accuracy: 0.3807 - lr: 0.0059\n",
            "Epoch 47/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0898 - accuracy: 0.3812\n",
            "Epoch 47: val_accuracy improved from 0.38330 to 0.38569, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0892 - accuracy: 0.3813 - val_loss: 3.1552 - val_accuracy: 0.3857 - lr: 0.0059\n",
            "Epoch 48/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0861 - accuracy: 0.3836\n",
            "Epoch 48: val_accuracy did not improve from 0.38569\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0854 - accuracy: 0.3837 - val_loss: 3.1521 - val_accuracy: 0.3850 - lr: 0.0059\n",
            "Epoch 49/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0752 - accuracy: 0.3897\n",
            "Epoch 49: val_accuracy improved from 0.38569 to 0.38809, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0742 - accuracy: 0.3900 - val_loss: 3.1464 - val_accuracy: 0.3881 - lr: 0.0059\n",
            "Epoch 50/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0710 - accuracy: 0.3890\n",
            "Epoch 50: val_accuracy did not improve from 0.38809\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0704 - accuracy: 0.3891 - val_loss: 3.1400 - val_accuracy: 0.3881 - lr: 0.0059\n",
            "Epoch 51/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0647 - accuracy: 0.3908\n",
            "Epoch 51: val_accuracy did not improve from 0.38809\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0638 - accuracy: 0.3910 - val_loss: 3.1364 - val_accuracy: 0.3880 - lr: 0.0059\n",
            "Epoch 52/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0577 - accuracy: 0.3935\n",
            "Epoch 52: val_accuracy improved from 0.38809 to 0.39211, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0571 - accuracy: 0.3937 - val_loss: 3.1316 - val_accuracy: 0.3921 - lr: 0.0059\n",
            "Epoch 53/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0497 - accuracy: 0.3945\n",
            "Epoch 53: val_accuracy did not improve from 0.39211\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0491 - accuracy: 0.3947 - val_loss: 3.1280 - val_accuracy: 0.3921 - lr: 0.0059\n",
            "Epoch 54/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0417 - accuracy: 0.3972\n",
            "Epoch 54: val_accuracy improved from 0.39211 to 0.39678, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0411 - accuracy: 0.3973 - val_loss: 3.1226 - val_accuracy: 0.3968 - lr: 0.0059\n",
            "Epoch 55/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0393 - accuracy: 0.3974\n",
            "Epoch 55: val_accuracy improved from 0.39678 to 0.39776, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0386 - accuracy: 0.3973 - val_loss: 3.1190 - val_accuracy: 0.3978 - lr: 0.0059\n",
            "Epoch 56/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0322 - accuracy: 0.4002\n",
            "Epoch 56: val_accuracy did not improve from 0.39776\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0316 - accuracy: 0.4004 - val_loss: 3.1113 - val_accuracy: 0.3969 - lr: 0.0059\n",
            "Epoch 57/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0230 - accuracy: 0.4028\n",
            "Epoch 57: val_accuracy did not improve from 0.39776\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0225 - accuracy: 0.4029 - val_loss: 3.1103 - val_accuracy: 0.3972 - lr: 0.0059\n",
            "Epoch 58/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0180 - accuracy: 0.4034\n",
            "Epoch 58: val_accuracy did not improve from 0.39776\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0174 - accuracy: 0.4035 - val_loss: 3.1030 - val_accuracy: 0.3967 - lr: 0.0059\n",
            "Epoch 59/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0156 - accuracy: 0.4034\n",
            "Epoch 59: val_accuracy improved from 0.39776 to 0.39830, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0147 - accuracy: 0.4035 - val_loss: 3.1015 - val_accuracy: 0.3983 - lr: 0.0059\n",
            "Epoch 60/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0084 - accuracy: 0.4084\n",
            "Epoch 60: val_accuracy improved from 0.39830 to 0.40157, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0075 - accuracy: 0.4084 - val_loss: 3.0985 - val_accuracy: 0.4016 - lr: 0.0059\n",
            "Epoch 61/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 3.0032 - accuracy: 0.4085\n",
            "Epoch 61: val_accuracy did not improve from 0.40157\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 3.0027 - accuracy: 0.4085 - val_loss: 3.0935 - val_accuracy: 0.4004 - lr: 0.0059\n",
            "Epoch 62/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9953 - accuracy: 0.4105\n",
            "Epoch 62: val_accuracy improved from 0.40157 to 0.40385, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9946 - accuracy: 0.4106 - val_loss: 3.0896 - val_accuracy: 0.4038 - lr: 0.0059\n",
            "Epoch 63/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.9914 - accuracy: 0.4121\n",
            "Epoch 63: val_accuracy did not improve from 0.40385\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9913 - accuracy: 0.4122 - val_loss: 3.0868 - val_accuracy: 0.4013 - lr: 0.0059\n",
            "Epoch 64/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9854 - accuracy: 0.4116\n",
            "Epoch 64: val_accuracy improved from 0.40385 to 0.40537, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9848 - accuracy: 0.4117 - val_loss: 3.0812 - val_accuracy: 0.4054 - lr: 0.0059\n",
            "Epoch 65/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9789 - accuracy: 0.4155\n",
            "Epoch 65: val_accuracy did not improve from 0.40537\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9786 - accuracy: 0.4154 - val_loss: 3.0776 - val_accuracy: 0.4038 - lr: 0.0059\n",
            "Epoch 66/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9746 - accuracy: 0.4161\n",
            "Epoch 66: val_accuracy improved from 0.40537 to 0.40580, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9739 - accuracy: 0.4162 - val_loss: 3.0758 - val_accuracy: 0.4058 - lr: 0.0058\n",
            "Epoch 67/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9701 - accuracy: 0.4181\n",
            "Epoch 67: val_accuracy improved from 0.40580 to 0.40928, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9697 - accuracy: 0.4182 - val_loss: 3.0738 - val_accuracy: 0.4093 - lr: 0.0058\n",
            "Epoch 68/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9665 - accuracy: 0.4185\n",
            "Epoch 68: val_accuracy did not improve from 0.40928\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9660 - accuracy: 0.4187 - val_loss: 3.0678 - val_accuracy: 0.4069 - lr: 0.0058\n",
            "Epoch 69/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9564 - accuracy: 0.4214\n",
            "Epoch 69: val_accuracy did not improve from 0.40928\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9560 - accuracy: 0.4215 - val_loss: 3.0666 - val_accuracy: 0.4090 - lr: 0.0058\n",
            "Epoch 70/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9548 - accuracy: 0.4237\n",
            "Epoch 70: val_accuracy improved from 0.40928 to 0.41004, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9543 - accuracy: 0.4237 - val_loss: 3.0621 - val_accuracy: 0.4100 - lr: 0.0058\n",
            "Epoch 71/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9484 - accuracy: 0.4235\n",
            "Epoch 71: val_accuracy did not improve from 0.41004\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9480 - accuracy: 0.4235 - val_loss: 3.0575 - val_accuracy: 0.4083 - lr: 0.0058\n",
            "Epoch 72/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9434 - accuracy: 0.4264\n",
            "Epoch 72: val_accuracy did not improve from 0.41004\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9429 - accuracy: 0.4266 - val_loss: 3.0588 - val_accuracy: 0.4095 - lr: 0.0058\n",
            "Epoch 73/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9374 - accuracy: 0.4269\n",
            "Epoch 73: val_accuracy improved from 0.41004 to 0.41233, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9368 - accuracy: 0.4270 - val_loss: 3.0516 - val_accuracy: 0.4123 - lr: 0.0058\n",
            "Epoch 74/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9306 - accuracy: 0.4292\n",
            "Epoch 74: val_accuracy did not improve from 0.41233\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9299 - accuracy: 0.4292 - val_loss: 3.0516 - val_accuracy: 0.4113 - lr: 0.0058\n",
            "Epoch 75/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9289 - accuracy: 0.4294\n",
            "Epoch 75: val_accuracy improved from 0.41233 to 0.41363, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9283 - accuracy: 0.4294 - val_loss: 3.0438 - val_accuracy: 0.4136 - lr: 0.0058\n",
            "Epoch 76/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9253 - accuracy: 0.4299\n",
            "Epoch 76: val_accuracy did not improve from 0.41363\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9245 - accuracy: 0.4299 - val_loss: 3.0434 - val_accuracy: 0.4112 - lr: 0.0058\n",
            "Epoch 77/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9168 - accuracy: 0.4321\n",
            "Epoch 77: val_accuracy did not improve from 0.41363\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9165 - accuracy: 0.4321 - val_loss: 3.0433 - val_accuracy: 0.4121 - lr: 0.0058\n",
            "Epoch 78/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9145 - accuracy: 0.4336\n",
            "Epoch 78: val_accuracy improved from 0.41363 to 0.41744, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9138 - accuracy: 0.4337 - val_loss: 3.0389 - val_accuracy: 0.4174 - lr: 0.0058\n",
            "Epoch 79/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9074 - accuracy: 0.4351\n",
            "Epoch 79: val_accuracy did not improve from 0.41744\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9069 - accuracy: 0.4350 - val_loss: 3.0349 - val_accuracy: 0.4154 - lr: 0.0058\n",
            "Epoch 80/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.9038 - accuracy: 0.4344\n",
            "Epoch 80: val_accuracy did not improve from 0.41744\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.9032 - accuracy: 0.4343 - val_loss: 3.0340 - val_accuracy: 0.4172 - lr: 0.0058\n",
            "Epoch 81/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8985 - accuracy: 0.4379\n",
            "Epoch 81: val_accuracy improved from 0.41744 to 0.41994, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8981 - accuracy: 0.4379 - val_loss: 3.0312 - val_accuracy: 0.4199 - lr: 0.0058\n",
            "Epoch 82/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8942 - accuracy: 0.4398\n",
            "Epoch 82: val_accuracy did not improve from 0.41994\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8938 - accuracy: 0.4397 - val_loss: 3.0269 - val_accuracy: 0.4181 - lr: 0.0058\n",
            "Epoch 83/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8888 - accuracy: 0.4409\n",
            "Epoch 83: val_accuracy did not improve from 0.41994\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8883 - accuracy: 0.4409 - val_loss: 3.0220 - val_accuracy: 0.4181 - lr: 0.0058\n",
            "Epoch 84/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8881 - accuracy: 0.4412\n",
            "Epoch 84: val_accuracy did not improve from 0.41994\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8876 - accuracy: 0.4413 - val_loss: 3.0222 - val_accuracy: 0.4171 - lr: 0.0058\n",
            "Epoch 85/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8806 - accuracy: 0.4408\n",
            "Epoch 85: val_accuracy did not improve from 0.41994\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8801 - accuracy: 0.4408 - val_loss: 3.0157 - val_accuracy: 0.4192 - lr: 0.0057\n",
            "Epoch 86/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8769 - accuracy: 0.4411\n",
            "Epoch 86: val_accuracy improved from 0.41994 to 0.42320, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8764 - accuracy: 0.4411 - val_loss: 3.0162 - val_accuracy: 0.4232 - lr: 0.0057\n",
            "Epoch 87/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8752 - accuracy: 0.4411\n",
            "Epoch 87: val_accuracy did not improve from 0.42320\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8744 - accuracy: 0.4411 - val_loss: 3.0161 - val_accuracy: 0.4230 - lr: 0.0057\n",
            "Epoch 88/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8675 - accuracy: 0.4450\n",
            "Epoch 88: val_accuracy did not improve from 0.42320\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8672 - accuracy: 0.4449 - val_loss: 3.0119 - val_accuracy: 0.4212 - lr: 0.0057\n",
            "Epoch 89/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8629 - accuracy: 0.4452\n",
            "Epoch 89: val_accuracy improved from 0.42320 to 0.42559, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8625 - accuracy: 0.4453 - val_loss: 3.0099 - val_accuracy: 0.4256 - lr: 0.0057\n",
            "Epoch 90/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8609 - accuracy: 0.4474\n",
            "Epoch 90: val_accuracy did not improve from 0.42559\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8605 - accuracy: 0.4474 - val_loss: 3.0084 - val_accuracy: 0.4225 - lr: 0.0057\n",
            "Epoch 91/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8552 - accuracy: 0.4498\n",
            "Epoch 91: val_accuracy did not improve from 0.42559\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8547 - accuracy: 0.4498 - val_loss: 3.0035 - val_accuracy: 0.4239 - lr: 0.0057\n",
            "Epoch 92/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8480 - accuracy: 0.4508\n",
            "Epoch 92: val_accuracy improved from 0.42559 to 0.42700, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8476 - accuracy: 0.4507 - val_loss: 3.0032 - val_accuracy: 0.4270 - lr: 0.0057\n",
            "Epoch 93/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8467 - accuracy: 0.4501\n",
            "Epoch 93: val_accuracy improved from 0.42700 to 0.42831, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8460 - accuracy: 0.4502 - val_loss: 3.0013 - val_accuracy: 0.4283 - lr: 0.0057\n",
            "Epoch 94/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8446 - accuracy: 0.4512\n",
            "Epoch 94: val_accuracy improved from 0.42831 to 0.42950, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8443 - accuracy: 0.4512 - val_loss: 2.9995 - val_accuracy: 0.4295 - lr: 0.0057\n",
            "Epoch 95/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8392 - accuracy: 0.4527\n",
            "Epoch 95: val_accuracy improved from 0.42950 to 0.43037, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8386 - accuracy: 0.4527 - val_loss: 2.9970 - val_accuracy: 0.4304 - lr: 0.0057\n",
            "Epoch 96/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8368 - accuracy: 0.4525\n",
            "Epoch 96: val_accuracy did not improve from 0.43037\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8362 - accuracy: 0.4524 - val_loss: 2.9951 - val_accuracy: 0.4296 - lr: 0.0057\n",
            "Epoch 97/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8297 - accuracy: 0.4546\n",
            "Epoch 97: val_accuracy improved from 0.43037 to 0.43070, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8292 - accuracy: 0.4546 - val_loss: 2.9923 - val_accuracy: 0.4307 - lr: 0.0057\n",
            "Epoch 98/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8283 - accuracy: 0.4539\n",
            "Epoch 98: val_accuracy did not improve from 0.43070\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8280 - accuracy: 0.4537 - val_loss: 2.9901 - val_accuracy: 0.4306 - lr: 0.0057\n",
            "Epoch 99/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8217 - accuracy: 0.4563\n",
            "Epoch 99: val_accuracy improved from 0.43070 to 0.43081, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8215 - accuracy: 0.4563 - val_loss: 2.9866 - val_accuracy: 0.4308 - lr: 0.0057\n",
            "Epoch 100/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8159 - accuracy: 0.4584\n",
            "Epoch 100: val_accuracy improved from 0.43081 to 0.43189, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8156 - accuracy: 0.4583 - val_loss: 2.9850 - val_accuracy: 0.4319 - lr: 0.0057\n",
            "Epoch 101/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8171 - accuracy: 0.4568\n",
            "Epoch 101: val_accuracy improved from 0.43189 to 0.43244, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8165 - accuracy: 0.4569 - val_loss: 2.9827 - val_accuracy: 0.4324 - lr: 0.0056\n",
            "Epoch 102/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8149 - accuracy: 0.4582\n",
            "Epoch 102: val_accuracy improved from 0.43244 to 0.43342, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8145 - accuracy: 0.4581 - val_loss: 2.9828 - val_accuracy: 0.4334 - lr: 0.0056\n",
            "Epoch 103/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8088 - accuracy: 0.4591\n",
            "Epoch 103: val_accuracy improved from 0.43342 to 0.43494, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8083 - accuracy: 0.4590 - val_loss: 2.9835 - val_accuracy: 0.4349 - lr: 0.0056\n",
            "Epoch 104/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8039 - accuracy: 0.4616\n",
            "Epoch 104: val_accuracy improved from 0.43494 to 0.43603, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8035 - accuracy: 0.4616 - val_loss: 2.9803 - val_accuracy: 0.4360 - lr: 0.0056\n",
            "Epoch 105/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.8018 - accuracy: 0.4618\n",
            "Epoch 105: val_accuracy did not improve from 0.43603\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.8014 - accuracy: 0.4618 - val_loss: 2.9759 - val_accuracy: 0.4359 - lr: 0.0056\n",
            "Epoch 106/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7993 - accuracy: 0.4650\n",
            "Epoch 106: val_accuracy did not improve from 0.43603\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7989 - accuracy: 0.4649 - val_loss: 2.9770 - val_accuracy: 0.4360 - lr: 0.0056\n",
            "Epoch 107/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7942 - accuracy: 0.4637\n",
            "Epoch 107: val_accuracy improved from 0.43603 to 0.43646, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7939 - accuracy: 0.4638 - val_loss: 2.9727 - val_accuracy: 0.4365 - lr: 0.0056\n",
            "Epoch 108/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7906 - accuracy: 0.4638\n",
            "Epoch 108: val_accuracy did not improve from 0.43646\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7901 - accuracy: 0.4637 - val_loss: 2.9720 - val_accuracy: 0.4325 - lr: 0.0056\n",
            "Epoch 109/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7898 - accuracy: 0.4649\n",
            "Epoch 109: val_accuracy did not improve from 0.43646\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7892 - accuracy: 0.4650 - val_loss: 2.9687 - val_accuracy: 0.4356 - lr: 0.0056\n",
            "Epoch 110/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7822 - accuracy: 0.4659\n",
            "Epoch 110: val_accuracy improved from 0.43646 to 0.43907, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7817 - accuracy: 0.4660 - val_loss: 2.9715 - val_accuracy: 0.4391 - lr: 0.0056\n",
            "Epoch 111/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7809 - accuracy: 0.4651\n",
            "Epoch 111: val_accuracy did not improve from 0.43907\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7806 - accuracy: 0.4651 - val_loss: 2.9650 - val_accuracy: 0.4387 - lr: 0.0056\n",
            "Epoch 112/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7753 - accuracy: 0.4672\n",
            "Epoch 112: val_accuracy did not improve from 0.43907\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7747 - accuracy: 0.4671 - val_loss: 2.9693 - val_accuracy: 0.4389 - lr: 0.0056\n",
            "Epoch 113/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7758 - accuracy: 0.4677\n",
            "Epoch 113: val_accuracy did not improve from 0.43907\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7752 - accuracy: 0.4677 - val_loss: 2.9652 - val_accuracy: 0.4386 - lr: 0.0056\n",
            "Epoch 114/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7722 - accuracy: 0.4685\n",
            "Epoch 114: val_accuracy did not improve from 0.43907\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7717 - accuracy: 0.4685 - val_loss: 2.9655 - val_accuracy: 0.4390 - lr: 0.0056\n",
            "Epoch 115/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7671 - accuracy: 0.4699\n",
            "Epoch 115: val_accuracy did not improve from 0.43907\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7668 - accuracy: 0.4698 - val_loss: 2.9599 - val_accuracy: 0.4365 - lr: 0.0055\n",
            "Epoch 116/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7640 - accuracy: 0.4702\n",
            "Epoch 116: val_accuracy did not improve from 0.43907\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7636 - accuracy: 0.4702 - val_loss: 2.9587 - val_accuracy: 0.4391 - lr: 0.0055\n",
            "Epoch 117/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7611 - accuracy: 0.4714\n",
            "Epoch 117: val_accuracy improved from 0.43907 to 0.44200, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7606 - accuracy: 0.4714 - val_loss: 2.9534 - val_accuracy: 0.4420 - lr: 0.0055\n",
            "Epoch 118/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7579 - accuracy: 0.4725\n",
            "Epoch 118: val_accuracy did not improve from 0.44200\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7575 - accuracy: 0.4724 - val_loss: 2.9554 - val_accuracy: 0.4414 - lr: 0.0055\n",
            "Epoch 119/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7568 - accuracy: 0.4717\n",
            "Epoch 119: val_accuracy improved from 0.44200 to 0.44211, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7564 - accuracy: 0.4718 - val_loss: 2.9562 - val_accuracy: 0.4421 - lr: 0.0055\n",
            "Epoch 120/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.7493 - accuracy: 0.4739\n",
            "Epoch 120: val_accuracy improved from 0.44211 to 0.44266, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7495 - accuracy: 0.4739 - val_loss: 2.9529 - val_accuracy: 0.4427 - lr: 0.0055\n",
            "Epoch 121/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.7489 - accuracy: 0.4753\n",
            "Epoch 121: val_accuracy improved from 0.44266 to 0.44483, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7490 - accuracy: 0.4753 - val_loss: 2.9506 - val_accuracy: 0.4448 - lr: 0.0055\n",
            "Epoch 122/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7448 - accuracy: 0.4762\n",
            "Epoch 122: val_accuracy did not improve from 0.44483\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7441 - accuracy: 0.4763 - val_loss: 2.9500 - val_accuracy: 0.4423 - lr: 0.0055\n",
            "Epoch 123/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7417 - accuracy: 0.4749\n",
            "Epoch 123: val_accuracy did not improve from 0.44483\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7412 - accuracy: 0.4749 - val_loss: 2.9493 - val_accuracy: 0.4397 - lr: 0.0055\n",
            "Epoch 124/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7392 - accuracy: 0.4763\n",
            "Epoch 124: val_accuracy did not improve from 0.44483\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7389 - accuracy: 0.4762 - val_loss: 2.9462 - val_accuracy: 0.4430 - lr: 0.0055\n",
            "Epoch 125/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7336 - accuracy: 0.4788\n",
            "Epoch 125: val_accuracy improved from 0.44483 to 0.44624, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7334 - accuracy: 0.4786 - val_loss: 2.9424 - val_accuracy: 0.4462 - lr: 0.0055\n",
            "Epoch 126/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7338 - accuracy: 0.4791\n",
            "Epoch 126: val_accuracy did not improve from 0.44624\n",
            "671/671 [==============================] - 6s 10ms/step - loss: 2.7335 - accuracy: 0.4791 - val_loss: 2.9443 - val_accuracy: 0.4449 - lr: 0.0055\n",
            "Epoch 127/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7300 - accuracy: 0.4796\n",
            "Epoch 127: val_accuracy did not improve from 0.44624\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7296 - accuracy: 0.4795 - val_loss: 2.9414 - val_accuracy: 0.4437 - lr: 0.0055\n",
            "Epoch 128/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7310 - accuracy: 0.4774\n",
            "Epoch 128: val_accuracy did not improve from 0.44624\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7307 - accuracy: 0.4774 - val_loss: 2.9413 - val_accuracy: 0.4437 - lr: 0.0054\n",
            "Epoch 129/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7264 - accuracy: 0.4806\n",
            "Epoch 129: val_accuracy improved from 0.44624 to 0.44701, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7261 - accuracy: 0.4804 - val_loss: 2.9405 - val_accuracy: 0.4470 - lr: 0.0054\n",
            "Epoch 130/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7205 - accuracy: 0.4823\n",
            "Epoch 130: val_accuracy did not improve from 0.44701\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7202 - accuracy: 0.4822 - val_loss: 2.9365 - val_accuracy: 0.4464 - lr: 0.0054\n",
            "Epoch 131/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7206 - accuracy: 0.4820\n",
            "Epoch 131: val_accuracy did not improve from 0.44701\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7202 - accuracy: 0.4819 - val_loss: 2.9399 - val_accuracy: 0.4469 - lr: 0.0054\n",
            "Epoch 132/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7202 - accuracy: 0.4810\n",
            "Epoch 132: val_accuracy did not improve from 0.44701\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7200 - accuracy: 0.4809 - val_loss: 2.9327 - val_accuracy: 0.4448 - lr: 0.0054\n",
            "Epoch 133/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7167 - accuracy: 0.4805\n",
            "Epoch 133: val_accuracy improved from 0.44701 to 0.44831, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7163 - accuracy: 0.4806 - val_loss: 2.9305 - val_accuracy: 0.4483 - lr: 0.0054\n",
            "Epoch 134/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7086 - accuracy: 0.4843\n",
            "Epoch 134: val_accuracy did not improve from 0.44831\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7081 - accuracy: 0.4844 - val_loss: 2.9334 - val_accuracy: 0.4449 - lr: 0.0054\n",
            "Epoch 135/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7078 - accuracy: 0.4847\n",
            "Epoch 135: val_accuracy did not improve from 0.44831\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7074 - accuracy: 0.4846 - val_loss: 2.9345 - val_accuracy: 0.4483 - lr: 0.0054\n",
            "Epoch 136/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7058 - accuracy: 0.4857\n",
            "Epoch 136: val_accuracy did not improve from 0.44831\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7053 - accuracy: 0.4856 - val_loss: 2.9314 - val_accuracy: 0.4467 - lr: 0.0054\n",
            "Epoch 137/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.7021 - accuracy: 0.4856\n",
            "Epoch 137: val_accuracy improved from 0.44831 to 0.44853, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.7017 - accuracy: 0.4855 - val_loss: 2.9299 - val_accuracy: 0.4485 - lr: 0.0054\n",
            "Epoch 138/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6994 - accuracy: 0.4869\n",
            "Epoch 138: val_accuracy improved from 0.44853 to 0.45027, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6990 - accuracy: 0.4867 - val_loss: 2.9291 - val_accuracy: 0.4503 - lr: 0.0054\n",
            "Epoch 139/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6948 - accuracy: 0.4881\n",
            "Epoch 139: val_accuracy did not improve from 0.45027\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6944 - accuracy: 0.4882 - val_loss: 2.9268 - val_accuracy: 0.4473 - lr: 0.0053\n",
            "Epoch 140/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6972 - accuracy: 0.4867\n",
            "Epoch 140: val_accuracy improved from 0.45027 to 0.45048, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6970 - accuracy: 0.4867 - val_loss: 2.9259 - val_accuracy: 0.4505 - lr: 0.0053\n",
            "Epoch 141/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6906 - accuracy: 0.4868\n",
            "Epoch 141: val_accuracy improved from 0.45048 to 0.45059, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6901 - accuracy: 0.4868 - val_loss: 2.9240 - val_accuracy: 0.4506 - lr: 0.0053\n",
            "Epoch 142/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6888 - accuracy: 0.4899\n",
            "Epoch 142: val_accuracy improved from 0.45059 to 0.45070, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6884 - accuracy: 0.4898 - val_loss: 2.9213 - val_accuracy: 0.4507 - lr: 0.0053\n",
            "Epoch 143/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6909 - accuracy: 0.4884\n",
            "Epoch 143: val_accuracy did not improve from 0.45070\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6905 - accuracy: 0.4885 - val_loss: 2.9205 - val_accuracy: 0.4504 - lr: 0.0053\n",
            "Epoch 144/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6846 - accuracy: 0.4906\n",
            "Epoch 144: val_accuracy improved from 0.45070 to 0.45081, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6843 - accuracy: 0.4905 - val_loss: 2.9162 - val_accuracy: 0.4508 - lr: 0.0053\n",
            "Epoch 145/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6783 - accuracy: 0.4930\n",
            "Epoch 145: val_accuracy improved from 0.45081 to 0.45353, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6780 - accuracy: 0.4930 - val_loss: 2.9172 - val_accuracy: 0.4535 - lr: 0.0053\n",
            "Epoch 146/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6796 - accuracy: 0.4921\n",
            "Epoch 146: val_accuracy did not improve from 0.45353\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6794 - accuracy: 0.4921 - val_loss: 2.9162 - val_accuracy: 0.4491 - lr: 0.0053\n",
            "Epoch 147/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6760 - accuracy: 0.4926\n",
            "Epoch 147: val_accuracy did not improve from 0.45353\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6754 - accuracy: 0.4927 - val_loss: 2.9173 - val_accuracy: 0.4505 - lr: 0.0053\n",
            "Epoch 148/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6729 - accuracy: 0.4916\n",
            "Epoch 148: val_accuracy did not improve from 0.45353\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6725 - accuracy: 0.4916 - val_loss: 2.9149 - val_accuracy: 0.4520 - lr: 0.0053\n",
            "Epoch 149/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6668 - accuracy: 0.4942\n",
            "Epoch 149: val_accuracy did not improve from 0.45353\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6663 - accuracy: 0.4943 - val_loss: 2.9167 - val_accuracy: 0.4500 - lr: 0.0053\n",
            "Epoch 150/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6700 - accuracy: 0.4933\n",
            "Epoch 150: val_accuracy did not improve from 0.45353\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6696 - accuracy: 0.4933 - val_loss: 2.9132 - val_accuracy: 0.4497 - lr: 0.0052\n",
            "Epoch 151/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6668 - accuracy: 0.4942\n",
            "Epoch 151: val_accuracy improved from 0.45353 to 0.45407, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6663 - accuracy: 0.4943 - val_loss: 2.9125 - val_accuracy: 0.4541 - lr: 0.0052\n",
            "Epoch 152/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6621 - accuracy: 0.4947\n",
            "Epoch 152: val_accuracy did not improve from 0.45407\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6617 - accuracy: 0.4947 - val_loss: 2.9131 - val_accuracy: 0.4527 - lr: 0.0052\n",
            "Epoch 153/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6568 - accuracy: 0.4985\n",
            "Epoch 153: val_accuracy improved from 0.45407 to 0.45559, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6563 - accuracy: 0.4986 - val_loss: 2.9057 - val_accuracy: 0.4556 - lr: 0.0052\n",
            "Epoch 154/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6552 - accuracy: 0.4974\n",
            "Epoch 154: val_accuracy improved from 0.45559 to 0.45581, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6547 - accuracy: 0.4974 - val_loss: 2.9105 - val_accuracy: 0.4558 - lr: 0.0052\n",
            "Epoch 155/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6556 - accuracy: 0.4963\n",
            "Epoch 155: val_accuracy did not improve from 0.45581\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6555 - accuracy: 0.4961 - val_loss: 2.9107 - val_accuracy: 0.4527 - lr: 0.0052\n",
            "Epoch 156/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6543 - accuracy: 0.4962\n",
            "Epoch 156: val_accuracy improved from 0.45581 to 0.45657, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6538 - accuracy: 0.4963 - val_loss: 2.9062 - val_accuracy: 0.4566 - lr: 0.0052\n",
            "Epoch 157/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6521 - accuracy: 0.4989\n",
            "Epoch 157: val_accuracy did not improve from 0.45657\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6519 - accuracy: 0.4989 - val_loss: 2.9049 - val_accuracy: 0.4545 - lr: 0.0052\n",
            "Epoch 158/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6484 - accuracy: 0.4992\n",
            "Epoch 158: val_accuracy did not improve from 0.45657\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6481 - accuracy: 0.4991 - val_loss: 2.9064 - val_accuracy: 0.4528 - lr: 0.0052\n",
            "Epoch 159/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6413 - accuracy: 0.5002\n",
            "Epoch 159: val_accuracy improved from 0.45657 to 0.45733, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6408 - accuracy: 0.5002 - val_loss: 2.9031 - val_accuracy: 0.4573 - lr: 0.0052\n",
            "Epoch 160/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.6464 - accuracy: 0.4982\n",
            "Epoch 160: val_accuracy did not improve from 0.45733\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6466 - accuracy: 0.4981 - val_loss: 2.9061 - val_accuracy: 0.4570 - lr: 0.0052\n",
            "Epoch 161/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6392 - accuracy: 0.5016\n",
            "Epoch 161: val_accuracy did not improve from 0.45733\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6386 - accuracy: 0.5016 - val_loss: 2.9052 - val_accuracy: 0.4537 - lr: 0.0051\n",
            "Epoch 162/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6354 - accuracy: 0.5015\n",
            "Epoch 162: val_accuracy did not improve from 0.45733\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6349 - accuracy: 0.5014 - val_loss: 2.9011 - val_accuracy: 0.4569 - lr: 0.0051\n",
            "Epoch 163/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6374 - accuracy: 0.5020\n",
            "Epoch 163: val_accuracy did not improve from 0.45733\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6371 - accuracy: 0.5021 - val_loss: 2.8982 - val_accuracy: 0.4570 - lr: 0.0051\n",
            "Epoch 164/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6369 - accuracy: 0.5023\n",
            "Epoch 164: val_accuracy improved from 0.45733 to 0.45820, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6364 - accuracy: 0.5024 - val_loss: 2.8987 - val_accuracy: 0.4582 - lr: 0.0051\n",
            "Epoch 165/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6314 - accuracy: 0.5026\n",
            "Epoch 165: val_accuracy did not improve from 0.45820\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6311 - accuracy: 0.5024 - val_loss: 2.8986 - val_accuracy: 0.4553 - lr: 0.0051\n",
            "Epoch 166/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6265 - accuracy: 0.5030\n",
            "Epoch 166: val_accuracy did not improve from 0.45820\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6263 - accuracy: 0.5029 - val_loss: 2.8965 - val_accuracy: 0.4569 - lr: 0.0051\n",
            "Epoch 167/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6251 - accuracy: 0.5046\n",
            "Epoch 167: val_accuracy did not improve from 0.45820\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6248 - accuracy: 0.5046 - val_loss: 2.8982 - val_accuracy: 0.4566 - lr: 0.0051\n",
            "Epoch 168/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6232 - accuracy: 0.5044\n",
            "Epoch 168: val_accuracy improved from 0.45820 to 0.46146, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6226 - accuracy: 0.5045 - val_loss: 2.8921 - val_accuracy: 0.4615 - lr: 0.0051\n",
            "Epoch 169/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.6236 - accuracy: 0.5065\n",
            "Epoch 169: val_accuracy did not improve from 0.46146\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6238 - accuracy: 0.5064 - val_loss: 2.8903 - val_accuracy: 0.4584 - lr: 0.0051\n",
            "Epoch 170/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6171 - accuracy: 0.5046\n",
            "Epoch 170: val_accuracy did not improve from 0.46146\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6166 - accuracy: 0.5046 - val_loss: 2.8934 - val_accuracy: 0.4585 - lr: 0.0051\n",
            "Epoch 171/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.6166 - accuracy: 0.5066\n",
            "Epoch 171: val_accuracy did not improve from 0.46146\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6168 - accuracy: 0.5065 - val_loss: 2.8987 - val_accuracy: 0.4606 - lr: 0.0050\n",
            "Epoch 172/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6160 - accuracy: 0.5080\n",
            "Epoch 172: val_accuracy did not improve from 0.46146\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6157 - accuracy: 0.5080 - val_loss: 2.8944 - val_accuracy: 0.4583 - lr: 0.0050\n",
            "Epoch 173/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6089 - accuracy: 0.5095\n",
            "Epoch 173: val_accuracy improved from 0.46146 to 0.46222, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6085 - accuracy: 0.5095 - val_loss: 2.8937 - val_accuracy: 0.4622 - lr: 0.0050\n",
            "Epoch 174/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6084 - accuracy: 0.5069\n",
            "Epoch 174: val_accuracy did not improve from 0.46222\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6081 - accuracy: 0.5070 - val_loss: 2.8916 - val_accuracy: 0.4616 - lr: 0.0050\n",
            "Epoch 175/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6065 - accuracy: 0.5093\n",
            "Epoch 175: val_accuracy did not improve from 0.46222\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6061 - accuracy: 0.5091 - val_loss: 2.8881 - val_accuracy: 0.4622 - lr: 0.0050\n",
            "Epoch 176/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6093 - accuracy: 0.5093\n",
            "Epoch 176: val_accuracy did not improve from 0.46222\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6092 - accuracy: 0.5092 - val_loss: 2.8891 - val_accuracy: 0.4596 - lr: 0.0050\n",
            "Epoch 177/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6032 - accuracy: 0.5089\n",
            "Epoch 177: val_accuracy did not improve from 0.46222\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6029 - accuracy: 0.5089 - val_loss: 2.8881 - val_accuracy: 0.4590 - lr: 0.0050\n",
            "Epoch 178/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6018 - accuracy: 0.5093\n",
            "Epoch 178: val_accuracy did not improve from 0.46222\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6014 - accuracy: 0.5093 - val_loss: 2.8871 - val_accuracy: 0.4614 - lr: 0.0050\n",
            "Epoch 179/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.6012 - accuracy: 0.5110\n",
            "Epoch 179: val_accuracy improved from 0.46222 to 0.46494, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.6006 - accuracy: 0.5110 - val_loss: 2.8846 - val_accuracy: 0.4649 - lr: 0.0050\n",
            "Epoch 180/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5951 - accuracy: 0.5105\n",
            "Epoch 180: val_accuracy did not improve from 0.46494\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5954 - accuracy: 0.5104 - val_loss: 2.8871 - val_accuracy: 0.4619 - lr: 0.0049\n",
            "Epoch 181/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5944 - accuracy: 0.5130\n",
            "Epoch 181: val_accuracy did not improve from 0.46494\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5940 - accuracy: 0.5130 - val_loss: 2.8843 - val_accuracy: 0.4624 - lr: 0.0049\n",
            "Epoch 182/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5968 - accuracy: 0.5124\n",
            "Epoch 182: val_accuracy did not improve from 0.46494\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5961 - accuracy: 0.5125 - val_loss: 2.8828 - val_accuracy: 0.4637 - lr: 0.0049\n",
            "Epoch 183/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5909 - accuracy: 0.5136\n",
            "Epoch 183: val_accuracy did not improve from 0.46494\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5904 - accuracy: 0.5137 - val_loss: 2.8836 - val_accuracy: 0.4644 - lr: 0.0049\n",
            "Epoch 184/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5907 - accuracy: 0.5116\n",
            "Epoch 184: val_accuracy did not improve from 0.46494\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5902 - accuracy: 0.5117 - val_loss: 2.8785 - val_accuracy: 0.4637 - lr: 0.0049\n",
            "Epoch 185/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5874 - accuracy: 0.5132\n",
            "Epoch 185: val_accuracy improved from 0.46494 to 0.46581, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5876 - accuracy: 0.5132 - val_loss: 2.8806 - val_accuracy: 0.4658 - lr: 0.0049\n",
            "Epoch 186/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5832 - accuracy: 0.5157\n",
            "Epoch 186: val_accuracy did not improve from 0.46581\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5827 - accuracy: 0.5157 - val_loss: 2.8832 - val_accuracy: 0.4626 - lr: 0.0049\n",
            "Epoch 187/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5846 - accuracy: 0.5146\n",
            "Epoch 187: val_accuracy did not improve from 0.46581\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5842 - accuracy: 0.5146 - val_loss: 2.8807 - val_accuracy: 0.4639 - lr: 0.0049\n",
            "Epoch 188/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5805 - accuracy: 0.5125\n",
            "Epoch 188: val_accuracy did not improve from 0.46581\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5799 - accuracy: 0.5125 - val_loss: 2.8790 - val_accuracy: 0.4610 - lr: 0.0049\n",
            "Epoch 189/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5777 - accuracy: 0.5150\n",
            "Epoch 189: val_accuracy did not improve from 0.46581\n",
            "671/671 [==============================] - 6s 10ms/step - loss: 2.5772 - accuracy: 0.5152 - val_loss: 2.8777 - val_accuracy: 0.4633 - lr: 0.0048\n",
            "Epoch 190/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5773 - accuracy: 0.5171\n",
            "Epoch 190: val_accuracy improved from 0.46581 to 0.46722, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5774 - accuracy: 0.5172 - val_loss: 2.8765 - val_accuracy: 0.4672 - lr: 0.0048\n",
            "Epoch 191/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5720 - accuracy: 0.5157\n",
            "Epoch 191: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5724 - accuracy: 0.5156 - val_loss: 2.8800 - val_accuracy: 0.4668 - lr: 0.0048\n",
            "Epoch 192/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5748 - accuracy: 0.5165\n",
            "Epoch 192: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5742 - accuracy: 0.5167 - val_loss: 2.8758 - val_accuracy: 0.4653 - lr: 0.0048\n",
            "Epoch 193/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5705 - accuracy: 0.5192\n",
            "Epoch 193: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5704 - accuracy: 0.5193 - val_loss: 2.8697 - val_accuracy: 0.4669 - lr: 0.0048\n",
            "Epoch 194/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5684 - accuracy: 0.5182\n",
            "Epoch 194: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5680 - accuracy: 0.5182 - val_loss: 2.8737 - val_accuracy: 0.4658 - lr: 0.0048\n",
            "Epoch 195/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5638 - accuracy: 0.5199\n",
            "Epoch 195: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5642 - accuracy: 0.5199 - val_loss: 2.8749 - val_accuracy: 0.4636 - lr: 0.0048\n",
            "Epoch 196/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5652 - accuracy: 0.5190\n",
            "Epoch 196: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 6s 10ms/step - loss: 2.5647 - accuracy: 0.5190 - val_loss: 2.8737 - val_accuracy: 0.4641 - lr: 0.0048\n",
            "Epoch 197/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5627 - accuracy: 0.5196\n",
            "Epoch 197: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5621 - accuracy: 0.5197 - val_loss: 2.8714 - val_accuracy: 0.4665 - lr: 0.0048\n",
            "Epoch 198/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5658 - accuracy: 0.5172\n",
            "Epoch 198: val_accuracy did not improve from 0.46722\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5653 - accuracy: 0.5173 - val_loss: 2.8739 - val_accuracy: 0.4635 - lr: 0.0047\n",
            "Epoch 199/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5586 - accuracy: 0.5203\n",
            "Epoch 199: val_accuracy improved from 0.46722 to 0.46777, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5582 - accuracy: 0.5203 - val_loss: 2.8705 - val_accuracy: 0.4678 - lr: 0.0047\n",
            "Epoch 200/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5571 - accuracy: 0.5213\n",
            "Epoch 200: val_accuracy improved from 0.46777 to 0.46940, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5568 - accuracy: 0.5214 - val_loss: 2.8682 - val_accuracy: 0.4694 - lr: 0.0047\n",
            "Epoch 201/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5588 - accuracy: 0.5206\n",
            "Epoch 201: val_accuracy did not improve from 0.46940\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5585 - accuracy: 0.5207 - val_loss: 2.8665 - val_accuracy: 0.4679 - lr: 0.0047\n",
            "Epoch 202/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5550 - accuracy: 0.5204\n",
            "Epoch 202: val_accuracy did not improve from 0.46940\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5548 - accuracy: 0.5204 - val_loss: 2.8686 - val_accuracy: 0.4670 - lr: 0.0047\n",
            "Epoch 203/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5520 - accuracy: 0.5208\n",
            "Epoch 203: val_accuracy did not improve from 0.46940\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5524 - accuracy: 0.5205 - val_loss: 2.8675 - val_accuracy: 0.4682 - lr: 0.0047\n",
            "Epoch 204/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5504 - accuracy: 0.5222\n",
            "Epoch 204: val_accuracy did not improve from 0.46940\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5502 - accuracy: 0.5221 - val_loss: 2.8702 - val_accuracy: 0.4684 - lr: 0.0047\n",
            "Epoch 205/500\n",
            "666/671 [============================>.] - ETA: 0s - loss: 2.5455 - accuracy: 0.5247\n",
            "Epoch 205: val_accuracy did not improve from 0.46940\n",
            "671/671 [==============================] - 6s 10ms/step - loss: 2.5459 - accuracy: 0.5246 - val_loss: 2.8714 - val_accuracy: 0.4674 - lr: 0.0047\n",
            "Epoch 206/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5449 - accuracy: 0.5226\n",
            "Epoch 206: val_accuracy did not improve from 0.46940\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5446 - accuracy: 0.5226 - val_loss: 2.8651 - val_accuracy: 0.4692 - lr: 0.0047\n",
            "Epoch 207/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5440 - accuracy: 0.5238\n",
            "Epoch 207: val_accuracy improved from 0.46940 to 0.46962, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5437 - accuracy: 0.5237 - val_loss: 2.8663 - val_accuracy: 0.4696 - lr: 0.0046\n",
            "Epoch 208/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5400 - accuracy: 0.5234\n",
            "Epoch 208: val_accuracy did not improve from 0.46962\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5397 - accuracy: 0.5234 - val_loss: 2.8657 - val_accuracy: 0.4676 - lr: 0.0046\n",
            "Epoch 209/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5402 - accuracy: 0.5238\n",
            "Epoch 209: val_accuracy did not improve from 0.46962\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5398 - accuracy: 0.5240 - val_loss: 2.8630 - val_accuracy: 0.4680 - lr: 0.0046\n",
            "Epoch 210/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5350 - accuracy: 0.5247\n",
            "Epoch 210: val_accuracy did not improve from 0.46962\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5347 - accuracy: 0.5248 - val_loss: 2.8607 - val_accuracy: 0.4686 - lr: 0.0046\n",
            "Epoch 211/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5370 - accuracy: 0.5251\n",
            "Epoch 211: val_accuracy improved from 0.46962 to 0.47201, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5365 - accuracy: 0.5252 - val_loss: 2.8627 - val_accuracy: 0.4720 - lr: 0.0046\n",
            "Epoch 212/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5377 - accuracy: 0.5261\n",
            "Epoch 212: val_accuracy did not improve from 0.47201\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5373 - accuracy: 0.5261 - val_loss: 2.8645 - val_accuracy: 0.4720 - lr: 0.0046\n",
            "Epoch 213/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5331 - accuracy: 0.5246\n",
            "Epoch 213: val_accuracy did not improve from 0.47201\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5327 - accuracy: 0.5247 - val_loss: 2.8607 - val_accuracy: 0.4685 - lr: 0.0046\n",
            "Epoch 214/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5287 - accuracy: 0.5250\n",
            "Epoch 214: val_accuracy did not improve from 0.47201\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5282 - accuracy: 0.5251 - val_loss: 2.8555 - val_accuracy: 0.4686 - lr: 0.0046\n",
            "Epoch 215/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5287 - accuracy: 0.5264\n",
            "Epoch 215: val_accuracy did not improve from 0.47201\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5282 - accuracy: 0.5265 - val_loss: 2.8623 - val_accuracy: 0.4697 - lr: 0.0046\n",
            "Epoch 216/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5293 - accuracy: 0.5276\n",
            "Epoch 216: val_accuracy did not improve from 0.47201\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5288 - accuracy: 0.5276 - val_loss: 2.8576 - val_accuracy: 0.4686 - lr: 0.0045\n",
            "Epoch 217/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5244 - accuracy: 0.5280\n",
            "Epoch 217: val_accuracy did not improve from 0.47201\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5241 - accuracy: 0.5279 - val_loss: 2.8546 - val_accuracy: 0.4691 - lr: 0.0045\n",
            "Epoch 218/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5218 - accuracy: 0.5277\n",
            "Epoch 218: val_accuracy improved from 0.47201 to 0.47309, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5214 - accuracy: 0.5276 - val_loss: 2.8578 - val_accuracy: 0.4731 - lr: 0.0045\n",
            "Epoch 219/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5205 - accuracy: 0.5290\n",
            "Epoch 219: val_accuracy did not improve from 0.47309\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5202 - accuracy: 0.5289 - val_loss: 2.8574 - val_accuracy: 0.4706 - lr: 0.0045\n",
            "Epoch 220/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5198 - accuracy: 0.5283\n",
            "Epoch 220: val_accuracy did not improve from 0.47309\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5193 - accuracy: 0.5283 - val_loss: 2.8576 - val_accuracy: 0.4715 - lr: 0.0045\n",
            "Epoch 221/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5184 - accuracy: 0.5301\n",
            "Epoch 221: val_accuracy did not improve from 0.47309\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5179 - accuracy: 0.5302 - val_loss: 2.8562 - val_accuracy: 0.4728 - lr: 0.0045\n",
            "Epoch 222/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5159 - accuracy: 0.5286\n",
            "Epoch 222: val_accuracy did not improve from 0.47309\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5156 - accuracy: 0.5286 - val_loss: 2.8550 - val_accuracy: 0.4730 - lr: 0.0045\n",
            "Epoch 223/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5154 - accuracy: 0.5282\n",
            "Epoch 223: val_accuracy improved from 0.47309 to 0.47331, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5151 - accuracy: 0.5284 - val_loss: 2.8523 - val_accuracy: 0.4733 - lr: 0.0045\n",
            "Epoch 224/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5128 - accuracy: 0.5314\n",
            "Epoch 224: val_accuracy did not improve from 0.47331\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5125 - accuracy: 0.5313 - val_loss: 2.8553 - val_accuracy: 0.4720 - lr: 0.0044\n",
            "Epoch 225/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5142 - accuracy: 0.5294\n",
            "Epoch 225: val_accuracy did not improve from 0.47331\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5139 - accuracy: 0.5294 - val_loss: 2.8550 - val_accuracy: 0.4726 - lr: 0.0044\n",
            "Epoch 226/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5111 - accuracy: 0.5317\n",
            "Epoch 226: val_accuracy improved from 0.47331 to 0.47407, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5109 - accuracy: 0.5318 - val_loss: 2.8530 - val_accuracy: 0.4741 - lr: 0.0044\n",
            "Epoch 227/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5074 - accuracy: 0.5321\n",
            "Epoch 227: val_accuracy did not improve from 0.47407\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5072 - accuracy: 0.5321 - val_loss: 2.8508 - val_accuracy: 0.4708 - lr: 0.0044\n",
            "Epoch 228/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5080 - accuracy: 0.5323\n",
            "Epoch 228: val_accuracy improved from 0.47407 to 0.47451, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5078 - accuracy: 0.5323 - val_loss: 2.8538 - val_accuracy: 0.4745 - lr: 0.0044\n",
            "Epoch 229/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5049 - accuracy: 0.5322\n",
            "Epoch 229: val_accuracy did not improve from 0.47451\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5045 - accuracy: 0.5323 - val_loss: 2.8554 - val_accuracy: 0.4741 - lr: 0.0044\n",
            "Epoch 230/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5068 - accuracy: 0.5332\n",
            "Epoch 230: val_accuracy did not improve from 0.47451\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5065 - accuracy: 0.5331 - val_loss: 2.8533 - val_accuracy: 0.4733 - lr: 0.0044\n",
            "Epoch 231/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.5064 - accuracy: 0.5338\n",
            "Epoch 231: val_accuracy improved from 0.47451 to 0.47549, saving model to model.h5\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.5061 - accuracy: 0.5339 - val_loss: 2.8527 - val_accuracy: 0.4755 - lr: 0.0044\n",
            "Epoch 232/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4999 - accuracy: 0.5347\n",
            "Epoch 232: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4995 - accuracy: 0.5349 - val_loss: 2.8525 - val_accuracy: 0.4733 - lr: 0.0044\n",
            "Epoch 233/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4986 - accuracy: 0.5337\n",
            "Epoch 233: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4983 - accuracy: 0.5338 - val_loss: 2.8519 - val_accuracy: 0.4749 - lr: 0.0043\n",
            "Epoch 234/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4973 - accuracy: 0.5337\n",
            "Epoch 234: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4972 - accuracy: 0.5337 - val_loss: 2.8542 - val_accuracy: 0.4733 - lr: 0.0043\n",
            "Epoch 235/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4974 - accuracy: 0.5343\n",
            "Epoch 235: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4971 - accuracy: 0.5343 - val_loss: 2.8469 - val_accuracy: 0.4719 - lr: 0.0043\n",
            "Epoch 236/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4892 - accuracy: 0.5366\n",
            "Epoch 236: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4888 - accuracy: 0.5366 - val_loss: 2.8478 - val_accuracy: 0.4755 - lr: 0.0043\n",
            "Epoch 237/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4956 - accuracy: 0.5350\n",
            "Epoch 237: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4951 - accuracy: 0.5351 - val_loss: 2.8467 - val_accuracy: 0.4745 - lr: 0.0043\n",
            "Epoch 238/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4873 - accuracy: 0.5373\n",
            "Epoch 238: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4869 - accuracy: 0.5373 - val_loss: 2.8511 - val_accuracy: 0.4752 - lr: 0.0043\n",
            "Epoch 239/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4862 - accuracy: 0.5369\n",
            "Epoch 239: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4859 - accuracy: 0.5369 - val_loss: 2.8500 - val_accuracy: 0.4745 - lr: 0.0043\n",
            "Epoch 240/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4875 - accuracy: 0.5360\n",
            "Epoch 240: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4870 - accuracy: 0.5362 - val_loss: 2.8469 - val_accuracy: 0.4734 - lr: 0.0043\n",
            "Epoch 241/500\n",
            "667/671 [============================>.] - ETA: 0s - loss: 2.4873 - accuracy: 0.5362Restoring model weights from the end of the best epoch: 231.\n",
            "\n",
            "Epoch 241: val_accuracy did not improve from 0.47549\n",
            "671/671 [==============================] - 7s 10ms/step - loss: 2.4870 - accuracy: 0.5361 - val_loss: 2.8491 - val_accuracy: 0.4739 - lr: 0.0042\n",
            "Epoch 241: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    batch_size = batch_size,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=[earlystopping_cb, mdlcheckpoint_cb, lrscheduler_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "qCsr5wei9NCI"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(model_save_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVIspEu_sHl6"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t92ZtpzmsCeh",
        "outputId": "4bc7cc77-f6fc-4d9e-c713-ae08d02bda1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144/144 [==============================] - 1s 3ms/step - loss: 2.8780 - accuracy: 0.4694\n",
            "Loss:  2.8780357837677\n",
            "Accuracy:  0.4693988561630249\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wATX2S3KsMJp"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "qC2G_yd8sLpN",
        "outputId": "33b9c199-9d11-4dc0-fb77-3bb1c86dec75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "785uRkLKsTNX",
        "outputId": "5cfbd5c9-7877-4cc5-92f5-445af86ad3f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WYgm2nxsXjo"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StViEdArsXGH",
        "outputId": "c7badd4b-5996-4d42-846c-2b6dd49d124e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144/144 [==============================] - 1s 8ms/step - loss: 2.8780 - accuracy: 0.4694\n",
            "0.4693988561630249\n"
          ]
        }
      ],
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = export_model.evaluate(test_dataset)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6mrs8XRsfFk"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "GtGFmzxpseKE"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    \"yahoo.com\",\n",
        "    \"forbes.com\",\n",
        "    \"draftkings.com/\",\n",
        "    \"pornhub.com\",\n",
        "    \"xvideos.com\",\n",
        "    \"google.com\",\n",
        "    \"amazon.com\",\n",
        "    \"facebook.com\",\n",
        "    \"fidelity.com\",\n",
        "    \"youtube.com\",\n",
        "    \"youporn.com\",\n",
        "    \"bellesa.co\"\n",
        "]\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "\n",
        "def tag_visible(element):\n",
        "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
        "        return False\n",
        "    if isinstance(element, Comment):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def text_from_html(text):\n",
        "      soup = BeautifulSoup(text, 'html.parser')\n",
        "      texts = soup.findAll(text=True)\n",
        "      visible_texts = filter(tag_visible, texts)\n",
        "      result = u\" \".join(t.strip().lower() for t in visible_texts if t.strip().isalpha())\n",
        "      return ' '.join(result.split())\n",
        "\n",
        "for i in range(len(examples)):\n",
        "  page = requests.get(f\"https://{examples[i]}\", timeout=3, headers = {\"Accept-Language\": \"en-US\"})\n",
        "  text = text_from_html(page.text)\n",
        "  examples[i] = examples[i].rsplit(\".\",1)[0] + ' ' + text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "id": "WSIM97uoSTIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598d929b-c6a0-4e6d-dd41-951233694832"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yahoo home mail news finance sports entertainment life search shopping mail mail mail news news finance finance sports sports entertainment entertainment life life shopping shopping politics celebrity lifestyle sports business entertainment health style science technology ads celebrity whbq politics business quartz video engadget celebrity variety deadline huffpost politics video business weather weather groningen today fri sat sun scoreboard mlb nba ncaab ncaaf nfl nhl trending yesterday today tomorrow chicago milwaukee milwaukee pittsburgh detroit minnesota aquarius aries cancer capricorn gemini leo libra pisces sagittarius scorpio taurus virgo twitter facebook instagram youtube tiktok advertise careers help feedback',\n",
              " 'forbes explore billionaires innovation ai cloud cybersecurity games healthcare science sustainability leadership careers cxo education forbeswomen money fintech investing markets retirement taxes business energy manufacturing media policy retail sportsmoney transportation entrepreneurs franchises lifestyle arts dining forbeslife spirits travel vices watches store vetted gear style coupons purple squarespace verizon lululemon dell chewy advisor wheels lists video newsletters latest featured tips corrections privacy terms adchoices subscribe breaking asia europe breaking by forbes popular readers forbes brandvoice money innovation lifestyle billionaires leadership billionaires innovation leadership money business lifestyle vetted coupons lists advisor store conferences newsletters products forbesbooks advertise sitemap investors education adchoices adchoices adchoices advertise investors',\n",
              " 'draftkings ',\n",
              " 'pornhub login verify resend upload photos videos upgrade home categories pornstars neuken community recommended hottest explore playlists channels random newest hottest hd carlacute views hd views recommended hd mermaidluvabbw views hd views playlists videos views videos views channels impregnate roommate orientation all gay transgender mature videos videos milf videos amateur videos ebony videos anal videos videos lesbian videos threesome videos japanese videos hentai videos videos public videos cartoon videos bondage videos creampie videos transgender videos gangbang videos videos masturbation videos impregnate roommate pornstars videos views videos views videos views videos views videos views videos views blondes brunettes piercings tattooed asian arab latin black indian white milfs mature contests popular april angel yinyleon march sinner february littlebaddiegirl lenahaze theeicecold socks tits ass pussy amateur dick hot hentai sex boobs exclusive content hd cancel anytime premium access hd views hd carlacute views hd alexandrawett views hd blacked views hd badcutegirl views hd views hd carlacute views hd reislin views hd views hd views yesterday hd free brazzers views hd views hd views hd reislin views hd views hd closeupfantasy views hd free views hd views hd curlyheadedfck views hd syndicete views hd views hd littlelaine views hd reislin views hd views hd views hd views hd views hd views hd views hd views hd 青蛇之勾引姐夫却是法海幻术大威天龙来收妖 hongkongdoll views hd excogi views hd views hd views hd views hd views hottest longest newest hd views hd views hd views hd views hd pooksi views hd views hd views hd views hd views hd views hd views hd views hd daisybabytw views hd views hd views hd free mylf views hd pussykagelove views hd views hd thefoxalina views hd views next ok information sitemap dmca advertise webmasters press faq feedback discover mobile more english deutsch français español italiano português polski русский 日本語 dutch czech',\n",
              " 'xvideos account search site site search categories netherlands straight netherlands history pornstars channels profiles games amateur anal arab asian asmr ass aunt bbw beach bi black blonde blowjob brunette casting caught cheating chubby compilation creampie cuckold cumshot dp dutch fisting gangbang gapes gay hijab indian interracial joi latina lesbian lingerie mature milf movie oiled pov redhead rough shemale shower solo squirting stockings teacher teen straight gay trans history views views views views toosoonforbbc views views raboduroxx views rawcouples views views views views swallowed views views views views dirtysarahcom views views views views views views views privatelove views views views views views bravomodels views views views views mygonzo views flormariaporn views carmenssexjournal views views views pervyrussia views views views views npleasures views views rocanrolnene views inkaporn views views views views views next subscriptions xvideos advertising',\n",
              " 'google inloggen cookies aanpassen afrikaans azərbaycan bosanski català čeština cymraeg dansk deutsch eesti euskara filipino gaeilge galego hrvatski indonesia isizulu íslenska italiano kiswahili latviešu lietuvių magyar melayu nederlands norsk polski română shqip slovenčina slovenščina suomi svenska türkçe ελληνικά беларуская български кыргызча македонски монгол русский српски українська ქართული հայերեն עברית اردو العربية فارسی አማርኛ ไทย ລາວ 한국어 日本語 简体中文 繁體中文 privacybeleid servicevoorwaarden',\n",
              " 'amazon ',\n",
              " 'facebook press alt nederlands frysk polski türkçe deutsch العربية español italiano messenger watch places games marketplace oculus portal instagram bulletin local fundraisers services groups about developers careers privacy cookies terms help settings',\n",
              " 'fidelity privacy security',\n",
              " 'youtube about press copyright creators advertise developers terms privacy',\n",
              " 'youporn ok retro games strip hindi more amateur category anal category category category category blonde category blowjob category brunette category cumshots category milf category pov category category more pornstar madeincanarias pornstar pornstar pornstar pornstar pornstar pornstar pornstar pornstar pornstar pornstar pornstar more brazzers channel channel mydirtyhobby channel channel puba channel channel channel channel channel channel channel channel recommended newest erotica collections story standard college instructional interracial massage romantic striptease fake bts casting gonzo parody reality fantasy cosplay fantasy fetish joi public taboo cuckold voyeur action action anal blowjob bondage bukkake dp femdom fingering fisting footjob foreplay handjob kissing masturbation penetration pissing rimming finish creampie cumshots facial squirting swallow stars ethnicity arab asian ebony european german indian japanese latina age twenties milf mature bbw muscle hair beards blonde brunette hairy redhead shaved extras panties pantyhose tattoos uncut details orientation lesbian gay bisexual trans number couples threesome gangbang orgy production amateur homemade pov sfw vintage webcam details asmr cartoon compilation funny hentai upgrade swyp upload english back auto off on back deutsch english español français italiano nederlands polski português pусский türkçe 日本語 videos recommended channels collections categories categories lesbian amateur mature anal milf more blonde blowjob anal amateur brunette story standard college instructional interracial massage romantic striptease fake bts casting gonzo parody reality fantasy cosplay fantasy fetish joi public taboo cuckold voyeur action action anal blowjob bondage bukkake dp femdom fingering fisting footjob foreplay handjob kissing masturbation penetration pissing rimming finish creampie cumshots facial squirting swallow stars ethnicity arab asian ebony european german indian japanese latina age twenties milf mature bbw muscle hair beards blonde brunette hairy redhead shaved extras panties pantyhose tattoos uncut details orientation lesbian gay bisexual trans number couples threesome gangbang orgy production amateur homemade pov sfw vintage webcam details asmr cartoon compilation funny hentai neuken erotica pornstars alphabetical soffie undo rank videos rank videos rank videos rank videos rank videos rank videos more madeincanarias more undo more brunette blowjob blonde amateur anal more brazzers erito stepmomwithboys sugarbabestv rating views rating duration date more more more by by herlittlebitch mandy by by saschaillyvich by arousal by wastelandstudios by saschaillyvich by by paulgarland by drunkenhunk contact faq sitemap advertise webmasters press dmca',\n",
              " 'bellesa store longest categories sensual passionate rough anal bondage homemade orgy story squirt channels vixen deeper tushy eroticax hardx sinfulxxx sexart deeplush all romance audio kink toys giveaway videos stories articles passionate rough rough passionate rough rough sensual passionate rough homemade anal squirt orgy story bondage trending amateur lesbian massage']"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = export_model.predict(examples)"
      ],
      "metadata": {
        "id": "hmseytWUSQyJ"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLpwpl71vdS1",
        "outputId": "a67ba28e-84e2-4b88-fec2-ffcd5937c3be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.22360317e-02, 2.44711206e-04, 7.64734577e-04, 2.34883642e-04,\n",
              "        9.69818153e-04, 3.70745547e-04, 4.92830994e-03, 7.56277528e-04,\n",
              "        1.80614404e-02, 2.76250939e-04, 1.00277190e-03, 4.30414546e-03,\n",
              "        3.79682146e-03, 7.86052842e-05, 5.97446217e-07, 5.42755530e-04,\n",
              "        8.82135238e-04, 8.57398612e-04, 3.07757611e-04, 2.74163787e-04,\n",
              "        2.50422279e-04, 7.42333068e-04, 5.16961932e-01, 6.21202663e-02,\n",
              "        2.37214568e-04, 3.84388957e-04, 7.92575825e-04, 3.31944488e-02,\n",
              "        2.37043858e-01, 7.73202395e-04, 6.13784511e-03, 3.55867145e-04,\n",
              "        7.30866741e-05, 5.60162007e-04, 7.76044326e-04, 1.53797810e-04,\n",
              "        2.47698161e-04, 1.51563509e-04, 1.92639942e-03, 5.97889489e-03,\n",
              "        8.25871539e-04, 4.62941352e-05, 1.28880522e-04, 3.20590916e-04,\n",
              "        7.73494830e-03, 6.70437366e-05, 1.11087225e-03, 6.98026386e-04,\n",
              "        6.30040478e-04, 7.03035295e-03, 1.30581111e-03, 6.35618751e-04,\n",
              "        8.33240862e-04, 6.78727578e-04, 1.40532720e-04, 3.63403087e-04,\n",
              "        1.04230037e-03, 2.40543784e-04, 5.84018766e-04, 2.74106540e-04,\n",
              "        5.93308476e-04, 9.41149238e-03, 5.04711119e-04, 9.11578245e-04,\n",
              "        8.44674767e-04, 1.33603258e-04, 3.95401701e-04, 1.54053225e-04,\n",
              "        4.99374117e-04, 3.71371512e-04, 1.13820063e-03, 1.65039834e-04,\n",
              "        1.54683468e-04, 2.78913154e-04],\n",
              "       [3.08451951e-02, 2.32422753e-05, 7.16855211e-05, 2.23611296e-05,\n",
              "        1.43347468e-04, 3.56514283e-05, 8.64293892e-04, 7.43887576e-05,\n",
              "        2.07295176e-03, 2.58311411e-05, 1.37812400e-04, 6.40775077e-04,\n",
              "        5.01076283e-04, 4.92762820e-06, 6.40121400e-10, 3.74141855e-05,\n",
              "        7.60877883e-05, 9.93338545e-05, 3.07340088e-05, 1.17526251e-05,\n",
              "        2.60377092e-05, 5.70873708e-05, 7.46226013e-01, 4.22877669e-02,\n",
              "        1.37294946e-05, 3.94894887e-05, 7.52240085e-05, 1.66731402e-02,\n",
              "        1.50115579e-01, 5.72920580e-05, 9.40198835e-04, 3.37445999e-05,\n",
              "        1.62994149e-06, 5.33181046e-05, 8.24161034e-05, 4.02032083e-06,\n",
              "        2.35416901e-05, 1.35449891e-05, 2.86167575e-04, 5.67338022e-04,\n",
              "        3.39446487e-05, 1.06543496e-07, 1.19634815e-05, 1.60232958e-05,\n",
              "        2.35328963e-03, 3.21492109e-07, 1.10367429e-04, 4.96687062e-05,\n",
              "        4.75649613e-05, 1.01980707e-03, 1.51855391e-04, 5.93930708e-05,\n",
              "        8.13546212e-05, 5.13371742e-05, 1.33620060e-05, 3.45718872e-05,\n",
              "        8.89222210e-05, 2.25894873e-05, 5.68052928e-05, 2.61198584e-05,\n",
              "        5.66212366e-05, 1.89273618e-03, 4.81084789e-05, 8.70848016e-05,\n",
              "        8.73567988e-05, 1.29652235e-05, 3.76147473e-05, 1.46776865e-05,\n",
              "        4.70785126e-05, 3.60526246e-05, 1.10469147e-04, 1.56205770e-05,\n",
              "        3.55599650e-06, 2.46475647e-05],\n",
              "       [2.53130905e-02, 1.70767854e-03, 5.52640669e-03, 1.69615168e-03,\n",
              "        3.45498510e-03, 2.51559378e-03, 1.71653237e-02, 4.55406681e-03,\n",
              "        5.03153428e-02, 2.01189634e-03, 1.04554007e-02, 1.15014436e-02,\n",
              "        1.88629255e-02, 1.30125368e-03, 3.54346558e-02, 3.24982032e-02,\n",
              "        1.02659063e-02, 5.76703483e-03, 2.33155629e-03, 7.96015654e-03,\n",
              "        1.52792258e-03, 4.61081928e-03, 2.82415655e-02, 2.48917211e-02,\n",
              "        4.12301859e-03, 1.72743504e-03, 4.68108663e-03, 1.06077008e-02,\n",
              "        5.10258637e-02, 4.18493338e-03, 4.32806462e-02, 2.54899194e-03,\n",
              "        1.74733326e-02, 3.94746801e-03, 4.12150379e-03, 2.20488682e-02,\n",
              "        1.78409694e-03, 9.87437321e-04, 7.64885545e-03, 3.36978436e-02,\n",
              "        3.74498367e-02, 5.39915264e-02, 9.06154979e-04, 1.26543976e-02,\n",
              "        5.71941547e-02, 2.52360180e-02, 7.11977016e-03, 1.31300632e-02,\n",
              "        1.09666009e-02, 7.37597942e-02, 4.82867099e-02, 4.42577340e-03,\n",
              "        5.35308197e-03, 3.64102833e-02, 9.82945785e-04, 2.50182371e-03,\n",
              "        7.96557683e-03, 1.68131164e-03, 3.77925811e-03, 1.95102266e-03,\n",
              "        3.76193738e-03, 2.36481763e-02, 3.38536454e-03, 6.42718235e-03,\n",
              "        5.15773008e-03, 1.00900314e-03, 2.80043646e-03, 1.09455653e-03,\n",
              "        3.60280462e-03, 2.53143813e-03, 7.72549724e-03, 1.18336536e-03,\n",
              "        6.21184055e-03, 1.91435509e-03],\n",
              "       [1.41002772e-06, 1.79225131e-16, 5.45895476e-16, 1.49482813e-16,\n",
              "        3.84635662e-14, 3.20665736e-16, 7.16704040e-13, 1.02014721e-15,\n",
              "        1.43089837e-12, 1.71755293e-16, 7.82413058e-15, 5.97314895e-13,\n",
              "        8.39083712e-14, 2.98440587e-19, 0.00000000e+00, 9.75116566e-20,\n",
              "        1.68781850e-16, 2.57099699e-15, 1.01701486e-16, 5.61603097e-20,\n",
              "        2.28807845e-16, 4.02676234e-16, 9.98288453e-01, 1.77193833e-05,\n",
              "        6.11748556e-19, 1.06985153e-15, 1.04943105e-15, 2.15990440e-06,\n",
              "        1.69017166e-03, 4.31479880e-16, 1.04809694e-11, 2.58849272e-16,\n",
              "        1.82489658e-24, 4.67624338e-16, 1.84038300e-15, 9.66968974e-24,\n",
              "        1.62243483e-16, 6.84750302e-17, 1.03672720e-13, 1.69529240e-14,\n",
              "        2.57857452e-19, 1.65926795e-33, 6.30189072e-17, 7.28922994e-20,\n",
              "        3.81251558e-11, 3.63742264e-29, 1.79265545e-15, 4.16634364e-17,\n",
              "        1.20173238e-17, 2.44115709e-13, 3.15371542e-17, 4.76543701e-16,\n",
              "        1.03393384e-15, 2.63622296e-18, 8.46011942e-17, 2.92114320e-16,\n",
              "        3.75507387e-16, 1.50519065e-16, 6.72062516e-16, 1.98142870e-16,\n",
              "        7.08032935e-16, 1.03338397e-11, 4.77224080e-16, 8.03654220e-16,\n",
              "        1.45414762e-15, 7.38006109e-17, 3.07248099e-16, 9.91664816e-17,\n",
              "        3.74266087e-16, 3.39416134e-16, 1.26496239e-15, 1.01460459e-16,\n",
              "        3.42018408e-22, 1.31539717e-16],\n",
              "       [8.92203208e-03, 2.67413441e-07, 8.71071620e-07, 2.46148232e-07,\n",
              "        3.08810809e-06, 4.27625508e-07, 2.48871256e-05, 9.87776616e-07,\n",
              "        4.91958890e-05, 2.91646955e-07, 3.49327638e-06, 1.85440276e-05,\n",
              "        1.30857588e-05, 2.05336583e-08, 1.08647582e-15, 9.22178813e-08,\n",
              "        7.71930615e-07, 1.51669610e-06, 2.64924296e-07, 3.45470283e-08,\n",
              "        2.71021463e-07, 6.89216279e-07, 8.58044803e-01, 1.38392225e-02,\n",
              "        5.67886467e-08, 5.26549456e-07, 1.02040713e-06, 3.73070617e-03,\n",
              "        1.14799641e-01, 6.71078226e-07, 1.19149292e-04, 3.96907723e-07,\n",
              "        1.05267817e-09, 6.59486830e-07, 1.14563773e-06, 2.72811751e-09,\n",
              "        2.62723574e-07, 1.29713612e-07, 8.00959879e-06, 1.06545667e-05,\n",
              "        1.64856459e-07, 8.64047115e-13, 1.19817059e-07, 4.79922591e-08,\n",
              "        2.63634807e-04, 2.54333880e-11, 1.59216756e-06, 5.63947026e-07,\n",
              "        3.24145390e-07, 3.47410132e-05, 9.37588879e-07, 7.22940285e-07,\n",
              "        1.09258417e-06, 2.81946996e-07, 1.40526055e-07, 4.11646823e-07,\n",
              "        9.58782039e-07, 2.48291144e-07, 7.40226142e-07, 3.01723247e-07,\n",
              "        7.87527256e-07, 8.75632541e-05, 6.03833541e-07, 1.10431574e-06,\n",
              "        1.21437790e-06, 1.33586340e-07, 4.49894401e-07, 1.59568117e-07,\n",
              "        5.71470309e-07, 4.36785541e-07, 1.47576327e-06, 1.69222659e-07,\n",
              "        4.37515491e-09, 2.56729891e-07],\n",
              "       [2.14810874e-02, 1.21260482e-05, 3.91671492e-05, 1.15575440e-05,\n",
              "        7.70143379e-05, 1.87894875e-05, 5.59138542e-04, 3.89906490e-05,\n",
              "        1.39944046e-03, 1.36860663e-05, 1.20073302e-04, 3.58746096e-04,\n",
              "        3.35019227e-04, 2.10057215e-06, 1.72185946e-10, 2.09062455e-05,\n",
              "        5.32706254e-05, 6.12869844e-05, 1.33414014e-05, 5.74026080e-06,\n",
              "        1.17340651e-05, 3.12338634e-05, 6.56346917e-01, 4.82165515e-02,\n",
              "        6.03657963e-06, 1.83666252e-05, 3.97040822e-05, 1.37135955e-02,\n",
              "        2.47177333e-01, 2.99568401e-05, 2.33215233e-03, 1.80964726e-05,\n",
              "        1.00850980e-06, 2.91675769e-05, 4.09014974e-05, 1.60615150e-06,\n",
              "        1.22761858e-05, 6.33504214e-06, 1.70241154e-04, 3.73603543e-04,\n",
              "        3.07103583e-05, 1.32291662e-08, 5.79288962e-06, 9.00606483e-06,\n",
              "        3.08327167e-03, 8.51567776e-08, 6.31816874e-05, 4.43294484e-05,\n",
              "        2.75579241e-05, 1.45063247e-03, 1.32490153e-04, 3.18314633e-05,\n",
              "        4.43421741e-05, 5.97310354e-05, 6.61867853e-06, 1.83153406e-05,\n",
              "        4.68304243e-05, 1.14658960e-05, 3.06433321e-05, 1.38174737e-05,\n",
              "        3.02330554e-05, 1.37767568e-03, 2.59632652e-05, 4.80705858e-05,\n",
              "        4.64925870e-05, 6.55772192e-06, 2.02767897e-05, 7.50082518e-06,\n",
              "        2.58274413e-05, 1.91627078e-05, 6.14981691e-05, 8.01361330e-06,\n",
              "        1.37553570e-06, 1.22460415e-05],\n",
              "       [2.53130905e-02, 1.70767854e-03, 5.52640669e-03, 1.69615168e-03,\n",
              "        3.45498510e-03, 2.51559378e-03, 1.71653237e-02, 4.55406681e-03,\n",
              "        5.03153428e-02, 2.01189634e-03, 1.04554007e-02, 1.15014436e-02,\n",
              "        1.88629255e-02, 1.30125368e-03, 3.54346558e-02, 3.24982032e-02,\n",
              "        1.02659063e-02, 5.76703483e-03, 2.33155629e-03, 7.96015654e-03,\n",
              "        1.52792258e-03, 4.61081928e-03, 2.82415655e-02, 2.48917211e-02,\n",
              "        4.12301859e-03, 1.72743504e-03, 4.68108663e-03, 1.06077008e-02,\n",
              "        5.10258637e-02, 4.18493338e-03, 4.32806462e-02, 2.54899194e-03,\n",
              "        1.74733326e-02, 3.94746801e-03, 4.12150379e-03, 2.20488682e-02,\n",
              "        1.78409694e-03, 9.87437321e-04, 7.64885545e-03, 3.36978436e-02,\n",
              "        3.74498367e-02, 5.39915264e-02, 9.06154979e-04, 1.26543976e-02,\n",
              "        5.71941547e-02, 2.52360180e-02, 7.11977016e-03, 1.31300632e-02,\n",
              "        1.09666009e-02, 7.37597942e-02, 4.82867099e-02, 4.42577340e-03,\n",
              "        5.35308197e-03, 3.64102833e-02, 9.82945785e-04, 2.50182371e-03,\n",
              "        7.96557683e-03, 1.68131164e-03, 3.77925811e-03, 1.95102266e-03,\n",
              "        3.76193738e-03, 2.36481763e-02, 3.38536454e-03, 6.42718235e-03,\n",
              "        5.15773008e-03, 1.00900314e-03, 2.80043646e-03, 1.09455653e-03,\n",
              "        3.60280462e-03, 2.53143813e-03, 7.72549724e-03, 1.18336536e-03,\n",
              "        6.21184055e-03, 1.91435509e-03],\n",
              "       [6.49380609e-02, 8.76757374e-04, 2.78994674e-03, 8.59833381e-04,\n",
              "        2.56344955e-03, 1.30491902e-03, 1.35368537e-02, 2.44130078e-03,\n",
              "        4.44101170e-02, 1.01006101e-03, 6.77088695e-03, 9.94375441e-03,\n",
              "        1.27098542e-02, 4.15771414e-04, 1.28225598e-04, 6.91996142e-03,\n",
              "        4.47114417e-03, 3.32521670e-03, 1.14258670e-03, 1.84274057e-03,\n",
              "        8.22887523e-04, 2.41202698e-03, 2.31112927e-01, 7.55004734e-02,\n",
              "        1.24653766e-03, 1.05233467e-03, 2.53396644e-03, 3.15058827e-02,\n",
              "        2.08073765e-01, 2.19431240e-03, 3.23693305e-02, 1.29574747e-03,\n",
              "        1.52717053e-03, 2.02237070e-03, 2.40183203e-03, 2.07616179e-03,\n",
              "        9.04682209e-04, 4.95007611e-04, 5.64961787e-03, 1.83173940e-02,\n",
              "        7.88270030e-03, 9.13636584e-04, 4.56760114e-04, 2.83711636e-03,\n",
              "        4.88917381e-02, 9.67672851e-04, 3.86123336e-03, 4.48088208e-03,\n",
              "        3.70884710e-03, 3.79312523e-02, 1.34318387e-02, 2.24827463e-03,\n",
              "        2.85510486e-03, 9.70185734e-03, 5.00693859e-04, 1.28980307e-03,\n",
              "        3.78676294e-03, 8.53424135e-04, 2.00291374e-03, 9.94954491e-04,\n",
              "        2.00139871e-03, 2.23380346e-02, 1.76799344e-03, 3.30301072e-03,\n",
              "        2.84089451e-03, 5.06235054e-04, 1.43094710e-03, 5.57167805e-04,\n",
              "        1.82517793e-03, 1.31908525e-03, 4.06764727e-03, 5.98841580e-04,\n",
              "        9.80682787e-04, 9.49537090e-04],\n",
              "       [2.43605897e-02, 1.78064057e-03, 5.67750074e-03, 1.77725125e-03,\n",
              "        3.50249698e-03, 2.60908855e-03, 1.70155913e-02, 4.66251373e-03,\n",
              "        4.72784266e-02, 2.09112721e-03, 1.10251550e-02, 1.17607713e-02,\n",
              "        1.86729450e-02, 1.43494306e-03, 4.43368778e-02, 3.61589193e-02,\n",
              "        1.04990192e-02, 5.92396082e-03, 2.45507318e-03, 8.30357429e-03,\n",
              "        1.60426507e-03, 4.75035049e-03, 2.45836303e-02, 2.29103435e-02,\n",
              "        4.35995683e-03, 1.81234756e-03, 4.81680082e-03, 1.00625698e-02,\n",
              "        4.59345467e-02, 4.23978176e-03, 3.71806249e-02, 2.64074514e-03,\n",
              "        1.85983833e-02, 4.06283559e-03, 4.28572064e-03, 2.40599886e-02,\n",
              "        1.86360942e-03, 1.03731325e-03, 7.66549073e-03, 3.37520130e-02,\n",
              "        3.72361280e-02, 6.50287420e-02, 9.56955191e-04, 1.31499246e-02,\n",
              "        5.55342995e-02, 2.90776901e-02, 7.25954305e-03, 1.27249863e-02,\n",
              "        1.11754993e-02, 6.59020394e-02, 4.53378782e-02, 4.55276854e-03,\n",
              "        5.50979748e-03, 3.42176482e-02, 1.03630766e-03, 2.59229075e-03,\n",
              "        8.19192082e-03, 1.75833260e-03, 3.88513366e-03, 2.02903338e-03,\n",
              "        3.88007215e-03, 2.26180181e-02, 3.49419774e-03, 6.60538347e-03,\n",
              "        5.30638313e-03, 1.06812676e-03, 2.89543835e-03, 1.14908232e-03,\n",
              "        3.70888761e-03, 2.62775784e-03, 7.95383658e-03, 1.24051131e-03,\n",
              "        6.75450917e-03, 1.99306919e-03],\n",
              "       [3.42278406e-02, 1.79542508e-03, 5.73722320e-03, 1.77922763e-03,\n",
              "        3.96574009e-03, 2.64461874e-03, 1.97340548e-02, 4.77133971e-03,\n",
              "        5.47455996e-02, 2.10105744e-03, 1.21171065e-02, 1.33516891e-02,\n",
              "        2.04956438e-02, 1.31141767e-03, 1.48054408e-02, 2.57328413e-02,\n",
              "        9.37393773e-03, 5.87688806e-03, 2.42280471e-03, 7.54714431e-03,\n",
              "        1.61982491e-03, 4.84810350e-03, 4.45383750e-02, 3.15217599e-02,\n",
              "        3.96453869e-03, 1.88444345e-03, 4.95604472e-03, 1.50467157e-02,\n",
              "        7.24263713e-02, 4.43174737e-03, 4.01095562e-02, 2.66076881e-03,\n",
              "        1.23912310e-02, 4.11673030e-03, 4.45014518e-03, 1.79473199e-02,\n",
              "        1.86917640e-03, 1.03858835e-03, 8.68999120e-03, 3.82442214e-02,\n",
              "        3.22071910e-02, 4.47446704e-02, 9.56435688e-04, 1.09979929e-02,\n",
              "        6.35314286e-02, 1.99105106e-02, 7.46797072e-03, 1.15741240e-02,\n",
              "        1.00457547e-02, 6.53119832e-02, 3.90949883e-02, 4.60002199e-03,\n",
              "        5.62667334e-03, 2.79359873e-02, 1.03591895e-03, 2.62188003e-03,\n",
              "        8.17079190e-03, 1.76498597e-03, 3.97320464e-03, 2.04211683e-03,\n",
              "        3.96134285e-03, 2.67346501e-02, 3.55636026e-03, 6.70118071e-03,\n",
              "        5.47880540e-03, 1.06229342e-03, 2.92348233e-03, 1.15092378e-03,\n",
              "        3.74653703e-03, 2.65723257e-03, 8.09361879e-03, 1.24212436e-03,\n",
              "        5.77325700e-03, 2.01089634e-03],\n",
              "       [1.71769818e-04, 1.40648986e-11, 4.25786108e-11, 1.17797144e-11,\n",
              "        6.48484266e-10, 2.37981752e-11, 5.32840083e-09, 6.98352765e-11,\n",
              "        1.31865860e-08, 1.38004374e-11, 2.93383928e-10, 9.65323643e-09,\n",
              "        2.27895836e-09, 1.65894357e-13, 3.68905935e-28, 3.78532485e-14,\n",
              "        5.48141003e-12, 6.59688068e-11, 9.51317497e-12, 6.54741831e-14,\n",
              "        1.49325934e-11, 3.90323329e-11, 9.89825070e-01, 9.30412498e-05,\n",
              "        4.80382010e-13, 6.25434912e-11, 7.88831292e-11, 1.08754648e-04,\n",
              "        9.80066415e-03, 3.06372983e-11, 1.17486101e-08, 1.98218282e-11,\n",
              "        2.14146241e-17, 3.50023725e-11, 1.06046456e-10, 2.86762898e-16,\n",
              "        1.26407296e-11, 5.73753718e-12, 3.23237503e-09, 1.60271663e-09,\n",
              "        1.61399011e-13, 1.71967878e-22, 5.48336550e-12, 1.02695020e-13,\n",
              "        6.37315111e-07, 9.61058050e-20, 1.10150840e-10, 5.27687312e-12,\n",
              "        2.01365790e-12, 4.81083284e-10, 4.32589836e-13, 3.94074842e-11,\n",
              "        7.36489966e-11, 7.50190112e-14, 6.86249365e-12, 2.21633892e-11,\n",
              "        4.47984427e-11, 1.24731033e-11, 4.64690647e-11, 1.50665140e-11,\n",
              "        6.35092604e-11, 2.38093332e-08, 3.53903261e-11, 6.09318568e-11,\n",
              "        8.88232404e-11, 5.82981363e-12, 2.29874522e-11, 7.67959134e-12,\n",
              "        2.85561193e-11, 2.41888523e-11, 9.03084343e-11, 7.99476197e-12,\n",
              "        3.35878231e-15, 1.26284634e-11],\n",
              "       [5.66805974e-02, 1.16237358e-03, 3.76159116e-03, 1.13317231e-03,\n",
              "        3.11438064e-03, 1.73251808e-03, 1.56082772e-02, 3.29480064e-03,\n",
              "        4.52942587e-02, 1.33989553e-03, 9.00537521e-03, 1.22155398e-02,\n",
              "        1.70691498e-02, 5.75201877e-04, 3.07523762e-04, 7.77323684e-03,\n",
              "        5.38309710e-03, 3.95938428e-03, 1.43811794e-03, 2.37972499e-03,\n",
              "        1.04163156e-03, 3.32902581e-03, 1.59891710e-01, 5.92811853e-02,\n",
              "        1.78705051e-03, 1.36900623e-03, 3.53698456e-03, 2.73217186e-02,\n",
              "        1.82390004e-01, 2.85556517e-03, 4.68591526e-02, 1.72106538e-03,\n",
              "        2.34545744e-03, 2.70103058e-03, 3.16003873e-03, 3.45461676e-03,\n",
              "        1.19363633e-03, 6.47948764e-04, 7.74220610e-03, 2.52416488e-02,\n",
              "        1.14965690e-02, 1.12215045e-03, 5.95279736e-04, 4.09959070e-03,\n",
              "        8.27721730e-02, 1.53758586e-03, 5.17182751e-03, 6.96820207e-03,\n",
              "        5.62213827e-03, 4.79290560e-02, 1.53758479e-02, 3.05439928e-03,\n",
              "        3.80569301e-03, 9.84222814e-03, 6.55867625e-04, 1.71281782e-03,\n",
              "        5.30303875e-03, 1.13188056e-03, 2.67331651e-03, 1.31665892e-03,\n",
              "        2.83012399e-03, 2.69454718e-02, 2.35617720e-03, 4.42495430e-03,\n",
              "        3.73488013e-03, 6.59732323e-04, 1.89992203e-03, 7.31104810e-04,\n",
              "        2.43697292e-03, 1.75041810e-03, 5.46542602e-03, 7.86639343e-04,\n",
              "        1.42935128e-03, 1.26356108e-03]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "-UPUoqRkveL-"
      },
      "outputs": [],
      "source": [
        "probs = tf.nn.softmax(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwy98I4c1whq",
        "outputId": "5d5c032e-3594-4e27-c853-2babd8c5607c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12, 74), dtype=float32, numpy=\n",
              "array([[0.01401195, 0.01330207, 0.01330899, 0.01330194, 0.01331172,\n",
              "        0.01330374, 0.01336452, 0.01330888, 0.01354119, 0.01330249,\n",
              "        0.01331216, 0.01335618, 0.0133494 , 0.01329986, 0.01329882,\n",
              "        0.01330603, 0.01331055, 0.01331022, 0.01330291, 0.01330246,\n",
              "        0.01330214, 0.01330869, 0.02230112, 0.01415114, 0.01330197,\n",
              "        0.01330393, 0.01330936, 0.01374767, 0.0168562 , 0.0133091 ,\n",
              "        0.01338069, 0.01330355, 0.01329979, 0.01330627, 0.01330914,\n",
              "        0.01330086, 0.01330211, 0.01330083, 0.01332446, 0.01337856,\n",
              "        0.0133098 , 0.01329943, 0.01330053, 0.01330308, 0.01340208,\n",
              "        0.01329971, 0.0133136 , 0.0133081 , 0.0133072 , 0.01339264,\n",
              "        0.01331619, 0.01330727, 0.0133099 , 0.01330784, 0.01330068,\n",
              "        0.01330365, 0.01331268, 0.01330201, 0.01330658, 0.01330246,\n",
              "        0.01330671, 0.01342457, 0.01330553, 0.01331094, 0.01331005,\n",
              "        0.01330059, 0.01330407, 0.01330086, 0.01330546, 0.01330375,\n",
              "        0.01331396, 0.01330101, 0.01330087, 0.01330252],\n",
              "       [0.01368238, 0.0132671 , 0.01326774, 0.01326709, 0.01326869,\n",
              "        0.01326726, 0.01327826, 0.01326778, 0.01329432, 0.01326713,\n",
              "        0.01326862, 0.01327529, 0.01327344, 0.01326685, 0.01326679,\n",
              "        0.01326729, 0.0132678 , 0.01326811, 0.0132672 , 0.01326694,\n",
              "        0.01326713, 0.01326755, 0.02798   , 0.01383984, 0.01326697,\n",
              "        0.01326731, 0.01326779, 0.01348984, 0.01541559, 0.01326755,\n",
              "        0.01327927, 0.01326724, 0.01326681, 0.0132675 , 0.01326788,\n",
              "        0.01326684, 0.0132671 , 0.01326697, 0.01327059, 0.01327432,\n",
              "        0.01326724, 0.01326679, 0.01326695, 0.013267  , 0.01329805,\n",
              "        0.01326679, 0.01326825, 0.01326745, 0.01326742, 0.01328032,\n",
              "        0.0132688 , 0.01326758, 0.01326787, 0.01326747, 0.01326697,\n",
              "        0.01326725, 0.01326797, 0.01326709, 0.01326754, 0.01326714,\n",
              "        0.01326754, 0.01329192, 0.01326743, 0.01326794, 0.01326795,\n",
              "        0.01326696, 0.01326729, 0.01326698, 0.01326741, 0.01326727,\n",
              "        0.01326825, 0.013267  , 0.01326684, 0.01326712],\n",
              "       [0.01367205, 0.01335309, 0.01340418, 0.01335294, 0.01337644,\n",
              "        0.01336388, 0.0135611 , 0.01339115, 0.01401819, 0.01335715,\n",
              "        0.01347041, 0.01348451, 0.01358414, 0.01334766, 0.01381113,\n",
              "        0.01377064, 0.01346786, 0.01340741, 0.01336142, 0.01343684,\n",
              "        0.01335069, 0.01339191, 0.01371214, 0.01366629, 0.01338538,\n",
              "        0.01335335, 0.01339285, 0.01347246, 0.01402815, 0.01338621,\n",
              "        0.01391992, 0.01336433, 0.01356528, 0.01338303, 0.01338536,\n",
              "        0.01362749, 0.01335411, 0.01334348, 0.01343266, 0.01378717,\n",
              "        0.01383899, 0.01406982, 0.01334239, 0.01350007, 0.01411495,\n",
              "        0.01367099, 0.01342556, 0.01350649, 0.0134773 , 0.01435072,\n",
              "        0.01398978, 0.01338943, 0.01340186, 0.01382461, 0.01334342,\n",
              "        0.0133637 , 0.01343692, 0.01335274, 0.01338078, 0.01335634,\n",
              "        0.01338055, 0.0136493 , 0.01337551, 0.01341626, 0.01339924,\n",
              "        0.01334376, 0.01336769, 0.01334491, 0.01337842, 0.0133641 ,\n",
              "        0.01343369, 0.01334609, 0.01341337, 0.01335585],\n",
              "       [0.01320738, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.03583994, 0.0132076 , 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320739, 0.0132297 , 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736, 0.01320736,\n",
              "        0.01320736, 0.01320736, 0.01320736, 0.01320736],\n",
              "       [0.01336241, 0.01324372, 0.01324373, 0.01324372, 0.01324376,\n",
              "        0.01324372, 0.01324405, 0.01324373, 0.01324437, 0.01324372,\n",
              "        0.01324377, 0.01324396, 0.01324389, 0.01324372, 0.01324372,\n",
              "        0.01324372, 0.01324373, 0.01324374, 0.01324372, 0.01324372,\n",
              "        0.01324372, 0.01324373, 0.0312359 , 0.01342828, 0.01324372,\n",
              "        0.01324373, 0.01324373, 0.01329322, 0.0148548 , 0.01324373,\n",
              "        0.0132453 , 0.01324372, 0.01324372, 0.01324373, 0.01324373,\n",
              "        0.01324372, 0.01324372, 0.01324372, 0.01324382, 0.01324386,\n",
              "        0.01324372, 0.01324372, 0.01324372, 0.01324372, 0.01324721,\n",
              "        0.01324372, 0.01324374, 0.01324373, 0.01324372, 0.01324418,\n",
              "        0.01324373, 0.01324373, 0.01324373, 0.01324372, 0.01324372,\n",
              "        0.01324372, 0.01324373, 0.01324372, 0.01324373, 0.01324372,\n",
              "        0.01324373, 0.01324488, 0.01324373, 0.01324373, 0.01324373,\n",
              "        0.01324372, 0.01324372, 0.01324372, 0.01324373, 0.01324372,\n",
              "        0.01324374, 0.01324372, 0.01324372, 0.01324372],\n",
              "       [0.01356747, 0.01327929, 0.01327965, 0.01327928, 0.01328015,\n",
              "        0.01327938, 0.01328656, 0.01327965, 0.01329773, 0.01327931,\n",
              "        0.01328073, 0.0132839 , 0.01328358, 0.01327916, 0.01327913,\n",
              "        0.01327941, 0.01327984, 0.01327994, 0.01327931, 0.01327921,\n",
              "        0.01327929, 0.01327955, 0.02559868, 0.01393509, 0.01327921,\n",
              "        0.01327937, 0.01327966, 0.01346249, 0.01700268, 0.01327953,\n",
              "        0.01331014, 0.01327937, 0.01327914, 0.01327952, 0.01327967,\n",
              "        0.01327915, 0.01327929, 0.01327921, 0.01328139, 0.01328409,\n",
              "        0.01327954, 0.01327913, 0.01327921, 0.01327925, 0.01332014,\n",
              "        0.01327913, 0.01327997, 0.01327972, 0.0132795 , 0.01329841,\n",
              "        0.01328089, 0.01327955, 0.01327972, 0.01327992, 0.01327922,\n",
              "        0.01327937, 0.01327975, 0.01327928, 0.01327954, 0.01327931,\n",
              "        0.01327953, 0.01329744, 0.01327947, 0.01327977, 0.01327975,\n",
              "        0.01327922, 0.0132794 , 0.01327923, 0.01327947, 0.01327938,\n",
              "        0.01327995, 0.01327924, 0.01327915, 0.01327929],\n",
              "       [0.01367205, 0.01335309, 0.01340418, 0.01335294, 0.01337644,\n",
              "        0.01336388, 0.0135611 , 0.01339115, 0.01401819, 0.01335715,\n",
              "        0.01347041, 0.01348451, 0.01358414, 0.01334766, 0.01381113,\n",
              "        0.01377064, 0.01346786, 0.01340741, 0.01336142, 0.01343684,\n",
              "        0.01335069, 0.01339191, 0.01371214, 0.01366629, 0.01338538,\n",
              "        0.01335335, 0.01339285, 0.01347246, 0.01402815, 0.01338621,\n",
              "        0.01391992, 0.01336433, 0.01356528, 0.01338303, 0.01338536,\n",
              "        0.01362749, 0.01335411, 0.01334348, 0.01343266, 0.01378717,\n",
              "        0.01383899, 0.01406982, 0.01334239, 0.01350007, 0.01411495,\n",
              "        0.01367099, 0.01342556, 0.01350649, 0.0134773 , 0.01435072,\n",
              "        0.01398978, 0.01338943, 0.01340186, 0.01382461, 0.01334342,\n",
              "        0.0133637 , 0.01343692, 0.01335274, 0.01338078, 0.01335634,\n",
              "        0.01338055, 0.0136493 , 0.01337551, 0.01341626, 0.01339924,\n",
              "        0.01334376, 0.01336769, 0.01334491, 0.01337842, 0.0133641 ,\n",
              "        0.01343369, 0.01334609, 0.01341337, 0.01335585],\n",
              "       [0.01421612, 0.01333397, 0.01335951, 0.01333375, 0.01335648,\n",
              "        0.01333969, 0.01350386, 0.01335485, 0.01392727, 0.01333575,\n",
              "        0.0134128 , 0.01345543, 0.01349269, 0.01332783, 0.013324  ,\n",
              "        0.0134148 , 0.01338199, 0.01336666, 0.01333752, 0.01334686,\n",
              "        0.01333326, 0.01335446, 0.0167861 , 0.01436707, 0.01333891,\n",
              "        0.01333632, 0.01335609, 0.0137487 , 0.01640379, 0.01335155,\n",
              "        0.01376058, 0.01333956, 0.01334265, 0.01334926, 0.01335433,\n",
              "        0.01334998, 0.01333435, 0.01332889, 0.01339777, 0.01356857,\n",
              "        0.01342772, 0.01333447, 0.01332838, 0.01336014, 0.01398982,\n",
              "        0.01333519, 0.01337383, 0.01338212, 0.01337179, 0.01383733,\n",
              "        0.01350244, 0.01335228, 0.01336038, 0.01345217, 0.01332896,\n",
              "        0.01333948, 0.01337283, 0.01333366, 0.013349  , 0.01333555,\n",
              "        0.01334898, 0.01362323, 0.01334586, 0.01336637, 0.01336019,\n",
              "        0.01332904, 0.01334137, 0.01332971, 0.01334663, 0.01333987,\n",
              "        0.01337659, 0.01333027, 0.01333536, 0.01333495],\n",
              "       [0.0136591 , 0.01335413, 0.01340627, 0.01335409, 0.01337714,\n",
              "        0.0133652 , 0.01355914, 0.01339267, 0.01397575, 0.01335828,\n",
              "        0.01347815, 0.01348807, 0.01358163, 0.01334951, 0.0139347 ,\n",
              "        0.0138212 , 0.01347107, 0.01340958, 0.01336314, 0.01344152,\n",
              "        0.01335177, 0.01339385, 0.01366214, 0.0136393 , 0.01338862,\n",
              "        0.01335455, 0.01339474, 0.01346519, 0.01395698, 0.01338701,\n",
              "        0.01383533, 0.01336562, 0.01358061, 0.01338464, 0.01338763,\n",
              "        0.01365499, 0.01335524, 0.01334421, 0.01343295, 0.01378798,\n",
              "        0.0138361 , 0.01422604, 0.01334313, 0.01350682, 0.01409161,\n",
              "        0.01372368, 0.0134275 , 0.01350108, 0.01348018, 0.01423846,\n",
              "        0.01394865, 0.0133912 , 0.01340402, 0.0137944 , 0.01334419,\n",
              "        0.01336497, 0.01344002, 0.01335383, 0.01338226, 0.01335745,\n",
              "        0.0133822 , 0.01363532, 0.01337703, 0.01341872, 0.0134013 ,\n",
              "        0.01334462, 0.01336903, 0.0133457 , 0.01337991, 0.01336545,\n",
              "        0.01343682, 0.01334692, 0.01342072, 0.01335697],\n",
              "       [0.01379441, 0.0133542 , 0.01340694, 0.01335398, 0.01338321,\n",
              "        0.01336554, 0.01359591, 0.013394  , 0.01408036, 0.01335828,\n",
              "        0.01349275, 0.01350942, 0.01360627, 0.01334773, 0.01352907,\n",
              "        0.01367772, 0.01345579, 0.01340881, 0.01336258, 0.01343123,\n",
              "        0.01335185, 0.01339503, 0.01393737, 0.01375713, 0.0133832 ,\n",
              "        0.01335539, 0.01339647, 0.01353234, 0.01433153, 0.01338945,\n",
              "        0.01387578, 0.01336576, 0.01349645, 0.01338523, 0.0133897 ,\n",
              "        0.01357164, 0.01335518, 0.01334409, 0.01344659, 0.01384992,\n",
              "        0.01376656, 0.01394024, 0.013343  , 0.01347766, 0.01420461,\n",
              "        0.01359831, 0.01343017, 0.01348542, 0.01346483, 0.01422993,\n",
              "        0.01386171, 0.0133917 , 0.01340546, 0.01370789, 0.01334406,\n",
              "        0.01336524, 0.01343961, 0.01335379, 0.01338331, 0.01335749,\n",
              "        0.01338315, 0.01369143, 0.01337773, 0.01341987, 0.01340348,\n",
              "        0.01334441, 0.01336927, 0.01334559, 0.01338028, 0.01336571,\n",
              "        0.01343857, 0.01334681, 0.01340742, 0.01335707],\n",
              "       [0.01321214, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.03554463, 0.0132111 , 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.0132113 , 0.01333997, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320988,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987, 0.01320987,\n",
              "        0.01320987, 0.01320987, 0.01320987, 0.01320987],\n",
              "       [0.01410274, 0.01334111, 0.01337584, 0.01334072, 0.01336718,\n",
              "        0.01334872, 0.01353524, 0.01336959, 0.01394307, 0.01334348,\n",
              "        0.01344616, 0.01348939, 0.01355503, 0.01333328, 0.01332972,\n",
              "        0.0134296 , 0.01339754, 0.01337848, 0.01334479, 0.01335737,\n",
              "        0.0133395 , 0.01337005, 0.01563606, 0.01413946, 0.01334945,\n",
              "        0.01334387, 0.01337283, 0.01369471, 0.01599183, 0.01336372,\n",
              "        0.01396491, 0.01334857, 0.01335691, 0.01336166, 0.01336779,\n",
              "        0.01337173, 0.01334153, 0.01333425, 0.01342919, 0.01366626,\n",
              "        0.0134797 , 0.01334058, 0.01333355, 0.01338036, 0.01447554,\n",
              "        0.01334612, 0.01339471, 0.0134188 , 0.01340075, 0.01397985,\n",
              "        0.01353209, 0.01336638, 0.01337643, 0.01345742, 0.01333436,\n",
              "        0.01334846, 0.01339647, 0.01334071, 0.01336129, 0.01334317,\n",
              "        0.01336338, 0.01368956, 0.01335705, 0.01338471, 0.01337548,\n",
              "        0.01333441, 0.01335096, 0.01333536, 0.01335813, 0.01334896,\n",
              "        0.01339865, 0.0133361 , 0.01334468, 0.01334246]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "rXlTB44Bvh9p"
      },
      "outputs": [],
      "source": [
        "res_args = tf.argmax(results, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TKmoLawvjkS",
        "outputId": "8c8f4122-da14-455d-d9a7-6550f7431c8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12,), dtype=int64, numpy=array([22, 22, 49, 22, 22, 22, 49, 22, 49, 28, 22, 28])>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "res_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtVUogwvl2u",
        "outputId": "ffa98934-e6f0-49f9-ac3e-227e6b059abb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22, 22, 49, 22, 22, 22, 49, 22, 49, 28, 22, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "res_args.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVg1nasYtIGR",
        "outputId": "0edd597a-7002-49c4-dfc2-2812eb572b8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['adv', 'aggressive', 'alcohol', 'anonvpn', 'automobile/bikes',\n",
              "       'automobile/boats', 'automobile/cars', 'automobile/planes', 'chat',\n",
              "       'costtraps', 'dating', 'downloads', 'drugs', 'dynamic',\n",
              "       'education/schools', 'finance/banking', 'finance/insurance',\n",
              "       'finance/moneylending', 'finance/other', 'finance/realestate',\n",
              "       'finance/trading', 'fortunetelling', 'forum', 'gamble', 'government',\n",
              "       'hacking', 'hobby/cooking', 'hobby/games-misc', 'hobby/games-online',\n",
              "       'hobby/gardening', 'hobby/pets', 'homestyle', 'hospitals',\n",
              "       'imagehosting', 'isp', 'jobsearch', 'library', 'military', 'models',\n",
              "       'movies', 'music', 'news', 'podcasts', 'politics', 'porn', 'radiotv',\n",
              "       'recreation/humor', 'recreation/martialarts', 'recreation/restaurants',\n",
              "       'recreation/sports', 'recreation/travel', 'recreation/wellness',\n",
              "       'redirector', 'religion', 'remotecontrol', 'ringtones',\n",
              "       'science/astronomy', 'science/chemistry', 'searchengines',\n",
              "       'sex/education', 'sex/lingerie', 'shopping', 'socialnet', 'spyware',\n",
              "       'tracker', 'updatesites', 'urlshortener', 'violence', 'warez',\n",
              "       'weapons', 'webmail', 'webphone', 'webradio', 'webtv'],\n",
              "      dtype='object', name='cat_name')"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sGuHLRPPrRM",
        "outputId": "92059fc1-ef6f-4aa3-8136-0921124dad61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yahoo : forum\n",
            "forbes : forum\n",
            "draftkings : recreation/sports\n",
            "pornhub : forum\n",
            "xvideos : forum\n",
            "google : forum\n",
            "amazon : recreation/sports\n",
            "facebook : forum\n",
            "fidelity : recreation/sports\n",
            "youtube : hobby/games-online\n",
            "youporn : forum\n",
            "bellesa : hobby/games-online\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(examples)):\n",
        "  print(f\"{examples[i].split()[0]} : {classes[res_args[i]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "UgY9W6MSP4z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4410941-daa5-423e-b3ca-5583ebfcf436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 376948\n",
            "-rw-r--r-- 1 root root 244626908 Apr 27 12:37 domain_final.csv\n",
            "lrwxrwxrwx 1 root root        61 Apr 28 00:17 dap_multiplexer.INFO -> dap_multiplexer.d7d755732304.root.log.INFO.20220428-001701.61\n",
            "srwxr-xr-x 1 root root         0 Apr 28 00:17 debugger_1oyuyx0jar\n",
            "-rw-r--r-- 1 root root      1518 Apr 28 00:17 dap_multiplexer.d7d755732304.root.log.INFO.20220428-001701.61\n",
            "drwxr-xr-x 3 root root      4096 Apr 28 00:27 python-languageserver-cancellation\n",
            "drwx------ 2 root root      4096 Apr 28 00:27 pyright-138-yOrrozG5dlSn\n",
            "drwx------ 2 root root      4096 Apr 28 00:27 pyright-138-sW2N3MFVO0TR\n",
            "srw------- 1 root root         0 Apr 28 00:28 drivefs_ipc.0\n",
            "srw------- 1 root root         0 Apr 28 00:28 drivefs_ipc.0_shell\n",
            "drwx------ 2 root root      4096 Apr 28 00:28 initgoogle_syslog_dir.0\n",
            "lrwxrwxrwx 1 root root        73 Apr 28 00:28 directoryprefetcher_binary.INFO -> directoryprefetcher_binary.d7d755732304.root.log.INFO.20220428-002828.332\n",
            "-rw-r--r-- 1 root root      1567 Apr 28 00:28 directoryprefetcher_binary.d7d755732304.root.log.INFO.20220428-002828.332\n",
            "-rw------- 1 root root  51008931 Apr 28 00:28 domain_final.zip\n",
            "-rw------- 1 root root       211 Apr 28 00:31 __autograph_generated_filega2basux.py\n",
            "-rw------- 1 root root       654 Apr 28 00:31 __autograph_generated_filerjdoxp57.py\n",
            "-rw------- 1 root root       775 Apr 28 00:31 __autograph_generated_filejgk_cvck.py\n",
            "-rw------- 1 root root       871 Apr 28 00:34 __autograph_generated_fileb566ogp7.py\n",
            "-rw------- 1 root root       864 Apr 28 00:34 __autograph_generated_filezjf67q6u.py\n",
            "drwxr-xr-x 2 root root      4096 Apr 28 00:46 __pycache__\n",
            "-rw------- 1 root root       880 Apr 28 00:46 __autograph_generated_filetdil0hji.py\n",
            "-rw-r--r-- 1 root root  42586168 Apr 28 00:49 model.h5.zip\n",
            "-rw-r--r-- 1 root root  47700920 Apr 28 01:33 model.h5\n"
          ]
        }
      ],
      "source": [
        "!ls -ltr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip model.h5.zip model.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF3z3X8yowG9",
        "outputId": "05e2b2f5-4ce1-4881-c411-d5b204b552cf"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: model.h5 (deflated 10%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /tmp/model.h5.zip /content/drive/MyDrive/Colab/pydomains/data/"
      ],
      "metadata": {
        "id": "Dxx2y9OyoyIg"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "swXCEh7wpRmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train.ipynb",
      "provenance": [],
      "mount_file_id": "1rs-sjVP8KP2QaFYzYXDp2HrMFq8KvymR",
      "authorship_tag": "ABX9TyNeNmthJgY/qsoNhYxRyDMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}